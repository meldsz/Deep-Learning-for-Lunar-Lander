{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './LunarLanderFramesPart1/LunarLanderFramesPart1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 150\n",
    "sample_rate = 0.5\n",
    "channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30904/30904 [02:14<00:00, 230.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30904\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    for img in tqdm(os.listdir(path)):  # iterate over each image in the directory\n",
    "        try:\n",
    "            action = os.path.splitext(os.path.basename(img))[0].split('_')[3] # extract the action from the filename\n",
    "            img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert the image to grayscale \n",
    "            img_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize the image array to normalize the data\n",
    "            training_data.append([img_array,action])  # add the image and the label to our training_data\n",
    "        except Exception as e:  # in the interest in keeping the output clean...\n",
    "            pass\n",
    "            #except OSError as e:\n",
    "            #    print(\"OSErrroBad img most likely\", e, os.path.join(path,img))\n",
    "            #except Exception as e:\n",
    "            #    print(\"general exception\", e, os.path.join(path,img))\n",
    "        \n",
    "create_training_data()\n",
    "\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_data = random.sample(training_data, int(len(training_data) * sample_rate)) # take a sample of data\n",
    "plt.imshow(training_data[0][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for features,label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "\n",
    "plt.imshow(X[0], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5176, 150, 150, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Labels for actions')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y)\n",
    "plt.title(\"Labels for actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "Shape of train X:  (5176, 150, 150, 1)\n",
      "Shape of train y:  (5176, 4)\n",
      "Shape of test X:  (2550, 150, 150, 1)\n",
      "Shape of test y:  (2550, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "train_labels = keras.utils.to_categorical(y_train, 4)\n",
    "test_labels = keras.utils.to_categorical(y_test, 4)\n",
    "print(train_labels)\n",
    "\n",
    "print(\"Shape of train X: \",X_train.shape)\n",
    "print(\"Shape of train y: \",train_labels.shape)\n",
    "print(\"Shape of test X: \",X_test.shape)\n",
    "print(\"Shape of test y: \",test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Lunar-Lander-64x2-CNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 150, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MMD\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), padding=\"same\", input_shape=(150, 150,..., activation=\"relu\", strides=(4, 4))`\n",
      "  import sys\n",
      "C:\\Users\\MMD\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")`\n",
      "  \n",
      "C:\\Users\\MMD\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (4, 4), padding=\"same\", activation=\"relu\", strides=(2, 2))`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\MMD\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\MMD\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", strides=(1, 1))`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\MMD\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 38, 38, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 38, 19, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 19, 10, 64)        16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 19, 5, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 19, 5, 64)         18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 19, 2, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1216)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               623104    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 726,308\n",
      "Trainable params: 726,308\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train/255\n",
    "input_shape = (IMG_SIZE,IMG_SIZE,channels)\n",
    "\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Conv2D(32, (8, 8), padding='same', subsample=(4, 4), input_shape=input_shape, activation='relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2), dim_ordering = 'th'))\n",
    "model1.add(Conv2D(64, (4, 4), padding='same', subsample=(2, 2), activation='relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2), dim_ordering = 'th'))\n",
    "model1.add(Conv2D(64, (3, 3), padding='same', subsample=(1, 1), activation='relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2), dim_ordering = 'th'))\n",
    "model1.add(Flatten())\n",
    "\n",
    "model1.add(Dense(512, activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(128, activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "\n",
    "model1.add(Dense(4, activation = 'softmax'))\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "model1.compile(loss='categorical_crossentropy', optimizer=adam(lr=1e-3), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = X/255.0\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# output layer\n",
    "\n",
    "\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam(lr=1e-3),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 38, 38, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 38, 19, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 19, 10, 64)        16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 19, 5, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 19, 5, 64)         18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 19, 2, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1216)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               623104    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 726,308\n",
      "Trainable params: 726,308\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MMD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 3882 samples, validate on 1294 samples\n",
      "Epoch 1/50\n",
      "3882/3882 [==============================] - ETA: 4:00 - loss: 10.4782 - acc: 0.25 - ETA: 2:15 - loss: 9.6388 - acc: 0.3281 - ETA: 1:39 - loss: 8.9894 - acc: 0.377 - ETA: 1:21 - loss: 8.8422 - acc: 0.396 - ETA: 1:09 - loss: 8.7728 - acc: 0.404 - ETA: 1:00 - loss: 8.6813 - acc: 0.414 - ETA: 54s - loss: 8.6066 - acc: 0.423 - ETA: 48s - loss: 8.5712 - acc: 0.42 - ETA: 44s - loss: 8.4790 - acc: 0.43 - ETA: 40s - loss: 8.2472 - acc: 0.45 - ETA: 37s - loss: 8.3501 - acc: 0.44 - ETA: 34s - loss: 8.4139 - acc: 0.44 - ETA: 31s - loss: 8.3294 - acc: 0.45 - ETA: 29s - loss: 8.3355 - acc: 0.45 - ETA: 27s - loss: 8.3425 - acc: 0.45 - ETA: 24s - loss: 8.3047 - acc: 0.45 - ETA: 22s - loss: 8.3447 - acc: 0.45 - ETA: 20s - loss: 8.3918 - acc: 0.45 - ETA: 18s - loss: 8.4207 - acc: 0.45 - ETA: 16s - loss: 8.4467 - acc: 0.45 - ETA: 15s - loss: 8.4585 - acc: 0.45 - ETA: 13s - loss: 8.4247 - acc: 0.45 - ETA: 11s - loss: 8.4471 - acc: 0.45 - ETA: 9s - loss: 8.4412 - acc: 0.4557 - ETA: 8s - loss: 8.4360 - acc: 0.456 - ETA: 6s - loss: 8.4554 - acc: 0.456 - ETA: 5s - loss: 8.4454 - acc: 0.457 - ETA: 3s - loss: 8.5126 - acc: 0.454 - ETA: 1s - loss: 8.5317 - acc: 0.453 - ETA: 0s - loss: 8.5159 - acc: 0.455 - 49s 13ms/step - loss: 8.5027 - acc: 0.4562 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 2/50\n",
      "3882/3882 [==============================] - ETA: 35s - loss: 8.9405 - acc: 0.44 - ETA: 35s - loss: 9.1924 - acc: 0.42 - ETA: 34s - loss: 8.6047 - acc: 0.46 - ETA: 32s - loss: 8.5942 - acc: 0.46 - ETA: 32s - loss: 8.7390 - acc: 0.45 - ETA: 30s - loss: 8.6887 - acc: 0.46 - ETA: 29s - loss: 8.6527 - acc: 0.46 - ETA: 28s - loss: 8.6257 - acc: 0.46 - ETA: 26s - loss: 8.5068 - acc: 0.47 - ETA: 25s - loss: 8.6131 - acc: 0.46 - ETA: 24s - loss: 8.6658 - acc: 0.46 - ETA: 23s - loss: 8.6677 - acc: 0.46 - ETA: 22s - loss: 8.6887 - acc: 0.46 - ETA: 20s - loss: 8.5717 - acc: 0.46 - ETA: 19s - loss: 8.5040 - acc: 0.47 - ETA: 18s - loss: 8.5076 - acc: 0.47 - ETA: 16s - loss: 8.4813 - acc: 0.47 - ETA: 15s - loss: 8.5208 - acc: 0.47 - ETA: 14s - loss: 8.5893 - acc: 0.46 - ETA: 13s - loss: 8.5942 - acc: 0.46 - ETA: 11s - loss: 8.6347 - acc: 0.46 - ETA: 10s - loss: 8.6486 - acc: 0.46 - ETA: 9s - loss: 8.6777 - acc: 0.4616 - ETA: 8s - loss: 8.6572 - acc: 0.462 - ETA: 6s - loss: 8.6484 - acc: 0.463 - ETA: 5s - loss: 8.6305 - acc: 0.464 - ETA: 4s - loss: 8.6653 - acc: 0.462 - ETA: 2s - loss: 8.6572 - acc: 0.462 - ETA: 1s - loss: 8.7191 - acc: 0.459 - ETA: 0s - loss: 8.7390 - acc: 0.457 - 44s 11ms/step - loss: 8.7483 - acc: 0.4572 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 3/50\n",
      "3882/3882 [==============================] - ETA: 38s - loss: 9.3183 - acc: 0.42 - ETA: 35s - loss: 9.2553 - acc: 0.42 - ETA: 34s - loss: 8.9405 - acc: 0.44 - ETA: 33s - loss: 8.8775 - acc: 0.44 - ETA: 32s - loss: 9.0664 - acc: 0.43 - ETA: 31s - loss: 9.1924 - acc: 0.42 - ETA: 29s - loss: 8.8506 - acc: 0.45 - ETA: 28s - loss: 8.8618 - acc: 0.45 - ETA: 27s - loss: 8.8985 - acc: 0.44 - ETA: 25s - loss: 8.9531 - acc: 0.44 - ETA: 24s - loss: 8.9291 - acc: 0.44 - ETA: 23s - loss: 8.8880 - acc: 0.44 - ETA: 22s - loss: 8.8921 - acc: 0.44 - ETA: 20s - loss: 8.9855 - acc: 0.44 - ETA: 19s - loss: 8.9657 - acc: 0.44 - ETA: 18s - loss: 8.9326 - acc: 0.44 - ETA: 16s - loss: 8.8664 - acc: 0.44 - ETA: 15s - loss: 8.8566 - acc: 0.45 - ETA: 14s - loss: 8.7715 - acc: 0.45 - ETA: 13s - loss: 8.7547 - acc: 0.45 - ETA: 11s - loss: 8.7693 - acc: 0.45 - ETA: 10s - loss: 8.7484 - acc: 0.45 - ETA: 9s - loss: 8.7677 - acc: 0.4558 - ETA: 7s - loss: 8.7907 - acc: 0.454 - ETA: 6s - loss: 8.8067 - acc: 0.453 - ETA: 5s - loss: 8.8022 - acc: 0.453 - ETA: 4s - loss: 8.8026 - acc: 0.453 - ETA: 2s - loss: 8.7671 - acc: 0.455 - ETA: 1s - loss: 8.7470 - acc: 0.457 - ETA: 0s - loss: 8.7409 - acc: 0.457 - 44s 11ms/step - loss: 8.7418 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 4/50\n",
      "3882/3882 [==============================] - ETA: 37s - loss: 8.6887 - acc: 0.46 - ETA: 36s - loss: 8.9405 - acc: 0.44 - ETA: 35s - loss: 8.8985 - acc: 0.44 - ETA: 34s - loss: 9.0350 - acc: 0.43 - ETA: 32s - loss: 9.0412 - acc: 0.43 - ETA: 30s - loss: 9.0664 - acc: 0.43 - ETA: 29s - loss: 9.1024 - acc: 0.43 - ETA: 28s - loss: 9.1451 - acc: 0.43 - ETA: 26s - loss: 9.0944 - acc: 0.43 - ETA: 25s - loss: 9.0916 - acc: 0.43 - ETA: 24s - loss: 9.1122 - acc: 0.43 - ETA: 23s - loss: 9.0454 - acc: 0.43 - ETA: 21s - loss: 9.0567 - acc: 0.43 - ETA: 20s - loss: 8.9495 - acc: 0.44 - ETA: 19s - loss: 8.9825 - acc: 0.44 - ETA: 18s - loss: 8.9405 - acc: 0.44 - ETA: 16s - loss: 8.9924 - acc: 0.44 - ETA: 15s - loss: 8.9965 - acc: 0.44 - ETA: 14s - loss: 8.9206 - acc: 0.44 - ETA: 13s - loss: 8.9342 - acc: 0.44 - ETA: 11s - loss: 8.9105 - acc: 0.44 - ETA: 10s - loss: 8.9004 - acc: 0.44 - ETA: 9s - loss: 8.8693 - acc: 0.4497 - ETA: 8s - loss: 8.8461 - acc: 0.451 - ETA: 6s - loss: 8.8247 - acc: 0.452 - ETA: 5s - loss: 8.8097 - acc: 0.453 - ETA: 4s - loss: 8.7679 - acc: 0.456 - ETA: 2s - loss: 8.7336 - acc: 0.458 - ETA: 1s - loss: 8.7321 - acc: 0.458 - ETA: 0s - loss: 8.7306 - acc: 0.458 - 44s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 5/50\n",
      "3882/3882 [==============================] - ETA: 36s - loss: 8.9405 - acc: 0.44 - ETA: 34s - loss: 8.7516 - acc: 0.45 - ETA: 34s - loss: 9.0664 - acc: 0.43 - ETA: 33s - loss: 9.0035 - acc: 0.44 - ETA: 31s - loss: 8.6635 - acc: 0.46 - ETA: 30s - loss: 8.6257 - acc: 0.46 - ETA: 29s - loss: 8.6167 - acc: 0.46 - ETA: 28s - loss: 8.6414 - acc: 0.46 - ETA: 26s - loss: 8.6467 - acc: 0.46 - ETA: 25s - loss: 8.7264 - acc: 0.45 - ETA: 24s - loss: 8.6887 - acc: 0.46 - ETA: 23s - loss: 8.7831 - acc: 0.45 - ETA: 21s - loss: 8.8049 - acc: 0.45 - ETA: 20s - loss: 8.8236 - acc: 0.45 - ETA: 19s - loss: 8.8817 - acc: 0.44 - ETA: 18s - loss: 8.8697 - acc: 0.44 - ETA: 16s - loss: 8.7553 - acc: 0.45 - ETA: 15s - loss: 8.7516 - acc: 0.45 - ETA: 14s - loss: 8.7748 - acc: 0.45 - ETA: 13s - loss: 8.7642 - acc: 0.45 - ETA: 11s - loss: 8.7726 - acc: 0.45 - ETA: 10s - loss: 8.8031 - acc: 0.45 - ETA: 9s - loss: 8.8091 - acc: 0.4535 - ETA: 7s - loss: 8.8461 - acc: 0.451 - ETA: 6s - loss: 8.8498 - acc: 0.450 - ETA: 5s - loss: 8.8243 - acc: 0.452 - ETA: 4s - loss: 8.8099 - acc: 0.453 - ETA: 2s - loss: 8.7471 - acc: 0.457 - ETA: 1s - loss: 8.6973 - acc: 0.460 - ETA: 0s - loss: 8.7180 - acc: 0.459 - 41s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 6/50\n",
      "3882/3882 [==============================] - ETA: 37s - loss: 9.0664 - acc: 0.43 - ETA: 36s - loss: 8.6257 - acc: 0.46 - ETA: 34s - loss: 8.6467 - acc: 0.46 - ETA: 32s - loss: 8.1535 - acc: 0.49 - ETA: 31s - loss: 8.2102 - acc: 0.49 - ETA: 30s - loss: 8.3109 - acc: 0.48 - ETA: 28s - loss: 8.3109 - acc: 0.48 - ETA: 27s - loss: 8.4368 - acc: 0.47 - ETA: 26s - loss: 8.4648 - acc: 0.47 - ETA: 24s - loss: 8.4872 - acc: 0.47 - ETA: 23s - loss: 8.5170 - acc: 0.47 - ETA: 22s - loss: 8.5522 - acc: 0.46 - ETA: 20s - loss: 8.6209 - acc: 0.46 - ETA: 19s - loss: 8.6707 - acc: 0.46 - ETA: 18s - loss: 8.6635 - acc: 0.46 - ETA: 17s - loss: 8.7201 - acc: 0.45 - ETA: 16s - loss: 8.7331 - acc: 0.45 - ETA: 14s - loss: 8.7586 - acc: 0.45 - ETA: 13s - loss: 8.7284 - acc: 0.45 - ETA: 12s - loss: 8.8083 - acc: 0.45 - ETA: 11s - loss: 8.8086 - acc: 0.45 - ETA: 10s - loss: 8.8489 - acc: 0.45 - ETA: 8s - loss: 8.8091 - acc: 0.4535 - ETA: 7s - loss: 8.7988 - acc: 0.454 - ETA: 6s - loss: 8.8297 - acc: 0.452 - ETA: 5s - loss: 8.8146 - acc: 0.453 - ETA: 4s - loss: 8.7959 - acc: 0.454 - ETA: 2s - loss: 8.7966 - acc: 0.454 - ETA: 1s - loss: 8.7538 - acc: 0.456 - ETA: 0s - loss: 8.7558 - acc: 0.456 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3882/3882 [==============================] - ETA: 37s - loss: 9.3183 - acc: 0.42 - ETA: 35s - loss: 9.8220 - acc: 0.39 - ETA: 34s - loss: 9.6121 - acc: 0.40 - ETA: 32s - loss: 9.5072 - acc: 0.41 - ETA: 31s - loss: 9.2679 - acc: 0.42 - ETA: 30s - loss: 9.2553 - acc: 0.42 - ETA: 29s - loss: 9.1564 - acc: 0.43 - ETA: 28s - loss: 9.1924 - acc: 0.42 - ETA: 26s - loss: 9.2483 - acc: 0.42 - ETA: 25s - loss: 9.1798 - acc: 0.43 - ETA: 24s - loss: 9.1122 - acc: 0.43 - ETA: 22s - loss: 9.1609 - acc: 0.43 - ETA: 21s - loss: 9.1827 - acc: 0.43 - ETA: 20s - loss: 9.1384 - acc: 0.43 - ETA: 19s - loss: 9.1420 - acc: 0.43 - ETA: 17s - loss: 9.0900 - acc: 0.43 - ETA: 16s - loss: 9.0146 - acc: 0.44 - ETA: 15s - loss: 8.9475 - acc: 0.44 - ETA: 14s - loss: 8.9670 - acc: 0.44 - ETA: 12s - loss: 8.9531 - acc: 0.44 - ETA: 11s - loss: 8.9765 - acc: 0.44 - ETA: 10s - loss: 8.9233 - acc: 0.44 - ETA: 9s - loss: 8.9077 - acc: 0.4474 - ETA: 7s - loss: 8.8880 - acc: 0.448 - ETA: 6s - loss: 8.8146 - acc: 0.453 - ETA: 5s - loss: 8.8291 - acc: 0.452 - ETA: 4s - loss: 8.8146 - acc: 0.453 - ETA: 2s - loss: 8.7786 - acc: 0.455 - ETA: 1s - loss: 8.7712 - acc: 0.455 - ETA: 0s - loss: 8.7390 - acc: 0.457 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 8/50\n",
      "3882/3882 [==============================] - ETA: 37s - loss: 8.1850 - acc: 0.49 - ETA: 35s - loss: 8.7516 - acc: 0.45 - ETA: 34s - loss: 8.6467 - acc: 0.46 - ETA: 32s - loss: 8.7516 - acc: 0.45 - ETA: 31s - loss: 8.7138 - acc: 0.45 - ETA: 30s - loss: 8.9195 - acc: 0.44 - ETA: 29s - loss: 8.9765 - acc: 0.44 - ETA: 28s - loss: 8.9248 - acc: 0.44 - ETA: 27s - loss: 8.7866 - acc: 0.45 - ETA: 25s - loss: 8.8901 - acc: 0.44 - ETA: 24s - loss: 8.8604 - acc: 0.45 - ETA: 23s - loss: 8.8146 - acc: 0.45 - ETA: 22s - loss: 8.7952 - acc: 0.45 - ETA: 20s - loss: 8.7966 - acc: 0.45 - ETA: 19s - loss: 8.8062 - acc: 0.45 - ETA: 18s - loss: 8.8303 - acc: 0.45 - ETA: 17s - loss: 8.8442 - acc: 0.45 - ETA: 15s - loss: 8.8356 - acc: 0.45 - ETA: 14s - loss: 8.8013 - acc: 0.45 - ETA: 13s - loss: 8.7579 - acc: 0.45 - ETA: 11s - loss: 8.7726 - acc: 0.45 - ETA: 10s - loss: 8.7516 - acc: 0.45 - ETA: 9s - loss: 8.7270 - acc: 0.4586 - ETA: 8s - loss: 8.7464 - acc: 0.457 - ETA: 6s - loss: 8.7793 - acc: 0.455 - ETA: 5s - loss: 8.7952 - acc: 0.454 - ETA: 4s - loss: 8.7913 - acc: 0.454 - ETA: 2s - loss: 8.7516 - acc: 0.457 - ETA: 1s - loss: 8.7451 - acc: 0.457 - ETA: 0s - loss: 8.7348 - acc: 0.458 - 45s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 9/50\n",
      "3882/3882 [==============================] - ETA: 38s - loss: 8.0590 - acc: 0.50 - ETA: 38s - loss: 9.0664 - acc: 0.43 - ETA: 36s - loss: 9.2763 - acc: 0.42 - ETA: 34s - loss: 9.4127 - acc: 0.41 - ETA: 33s - loss: 9.3435 - acc: 0.42 - ETA: 31s - loss: 9.2763 - acc: 0.42 - ETA: 30s - loss: 9.3003 - acc: 0.42 - ETA: 29s - loss: 9.0979 - acc: 0.43 - ETA: 27s - loss: 9.0524 - acc: 0.43 - ETA: 26s - loss: 8.8775 - acc: 0.44 - ETA: 24s - loss: 8.8375 - acc: 0.45 - ETA: 23s - loss: 8.6992 - acc: 0.46 - ETA: 22s - loss: 8.7274 - acc: 0.45 - ETA: 21s - loss: 8.6797 - acc: 0.46 - ETA: 19s - loss: 8.7390 - acc: 0.45 - ETA: 18s - loss: 8.7595 - acc: 0.45 - ETA: 17s - loss: 8.7701 - acc: 0.45 - ETA: 15s - loss: 8.7097 - acc: 0.45 - ETA: 14s - loss: 8.7814 - acc: 0.45 - ETA: 13s - loss: 8.7579 - acc: 0.45 - ETA: 12s - loss: 8.7186 - acc: 0.45 - ETA: 10s - loss: 8.7345 - acc: 0.45 - ETA: 9s - loss: 8.7708 - acc: 0.4558 - ETA: 8s - loss: 8.7936 - acc: 0.454 - ETA: 6s - loss: 8.7944 - acc: 0.454 - ETA: 5s - loss: 8.7952 - acc: 0.454 - ETA: 4s - loss: 8.7819 - acc: 0.455 - ETA: 3s - loss: 8.8011 - acc: 0.454 - ETA: 1s - loss: 8.7798 - acc: 0.455 - ETA: 0s - loss: 8.7432 - acc: 0.457 - 45s 12ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 10/50\n",
      "3882/3882 [==============================] - ETA: 37s - loss: 8.5627 - acc: 0.46 - ETA: 36s - loss: 9.1294 - acc: 0.43 - ETA: 35s - loss: 9.0245 - acc: 0.44 - ETA: 34s - loss: 8.7831 - acc: 0.45 - ETA: 32s - loss: 8.7138 - acc: 0.45 - ETA: 31s - loss: 8.6467 - acc: 0.46 - ETA: 30s - loss: 8.6167 - acc: 0.46 - ETA: 29s - loss: 8.7988 - acc: 0.45 - ETA: 27s - loss: 8.7446 - acc: 0.45 - ETA: 26s - loss: 8.7642 - acc: 0.45 - ETA: 24s - loss: 8.8031 - acc: 0.45 - ETA: 23s - loss: 8.7726 - acc: 0.45 - ETA: 22s - loss: 8.7565 - acc: 0.45 - ETA: 21s - loss: 8.7876 - acc: 0.45 - ETA: 19s - loss: 8.8733 - acc: 0.44 - ETA: 18s - loss: 8.9012 - acc: 0.44 - ETA: 17s - loss: 8.9775 - acc: 0.44 - ETA: 16s - loss: 8.9405 - acc: 0.44 - ETA: 14s - loss: 8.8941 - acc: 0.44 - ETA: 13s - loss: 8.8838 - acc: 0.44 - ETA: 12s - loss: 8.8326 - acc: 0.45 - ETA: 10s - loss: 8.7860 - acc: 0.45 - ETA: 9s - loss: 8.7653 - acc: 0.4562 - ETA: 8s - loss: 8.7201 - acc: 0.459 - ETA: 6s - loss: 8.7340 - acc: 0.458 - ETA: 5s - loss: 8.7855 - acc: 0.454 - ETA: 4s - loss: 8.7586 - acc: 0.456 - ETA: 3s - loss: 8.7741 - acc: 0.455 - ETA: 1s - loss: 8.7755 - acc: 0.455 - ETA: 0s - loss: 8.7390 - acc: 0.457 - 45s 12ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 11/50\n",
      "3882/3882 [==============================] - ETA: 39s - loss: 9.3183 - acc: 0.42 - ETA: 36s - loss: 9.1294 - acc: 0.43 - ETA: 35s - loss: 8.5627 - acc: 0.46 - ETA: 34s - loss: 8.6572 - acc: 0.46 - ETA: 32s - loss: 8.5879 - acc: 0.46 - ETA: 31s - loss: 8.5627 - acc: 0.46 - ETA: 30s - loss: 8.6707 - acc: 0.46 - ETA: 28s - loss: 8.7044 - acc: 0.46 - ETA: 27s - loss: 8.7866 - acc: 0.45 - ETA: 25s - loss: 8.8398 - acc: 0.45 - ETA: 24s - loss: 8.8146 - acc: 0.45 - ETA: 23s - loss: 8.8146 - acc: 0.45 - ETA: 22s - loss: 8.7952 - acc: 0.45 - ETA: 20s - loss: 8.6977 - acc: 0.46 - ETA: 19s - loss: 8.6971 - acc: 0.46 - ETA: 18s - loss: 8.8067 - acc: 0.45 - ETA: 16s - loss: 8.8072 - acc: 0.45 - ETA: 15s - loss: 8.6957 - acc: 0.46 - ETA: 14s - loss: 8.6953 - acc: 0.46 - ETA: 13s - loss: 8.6698 - acc: 0.46 - ETA: 11s - loss: 8.6467 - acc: 0.46 - ETA: 10s - loss: 8.6486 - acc: 0.46 - ETA: 9s - loss: 8.6503 - acc: 0.4633 - ETA: 8s - loss: 8.6624 - acc: 0.462 - ETA: 6s - loss: 8.7088 - acc: 0.459 - ETA: 5s - loss: 8.6499 - acc: 0.463 - ETA: 4s - loss: 8.6653 - acc: 0.462 - ETA: 2s - loss: 8.6932 - acc: 0.460 - ETA: 1s - loss: 8.7668 - acc: 0.456 - ETA: 0s - loss: 8.7432 - acc: 0.457 - 44s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 12/50\n",
      "3882/3882 [==============================] - ETA: 36s - loss: 9.5701 - acc: 0.40 - ETA: 35s - loss: 9.3183 - acc: 0.42 - ETA: 33s - loss: 8.8566 - acc: 0.45 - ETA: 32s - loss: 8.7516 - acc: 0.45 - ETA: 30s - loss: 8.8650 - acc: 0.45 - ETA: 29s - loss: 8.7516 - acc: 0.45 - ETA: 27s - loss: 8.9225 - acc: 0.44 - ETA: 26s - loss: 8.9405 - acc: 0.44 - ETA: 24s - loss: 8.9405 - acc: 0.44 - ETA: 23s - loss: 8.9279 - acc: 0.44 - ETA: 22s - loss: 8.9291 - acc: 0.44 - ETA: 20s - loss: 8.8566 - acc: 0.45 - ETA: 19s - loss: 8.7952 - acc: 0.45 - ETA: 18s - loss: 8.8416 - acc: 0.45 - ETA: 17s - loss: 8.8230 - acc: 0.45 - ETA: 16s - loss: 8.8225 - acc: 0.45 - ETA: 15s - loss: 8.8516 - acc: 0.45 - ETA: 14s - loss: 8.8915 - acc: 0.44 - ETA: 13s - loss: 8.8941 - acc: 0.44 - ETA: 11s - loss: 8.8461 - acc: 0.45 - ETA: 10s - loss: 8.8626 - acc: 0.45 - ETA: 9s - loss: 8.8432 - acc: 0.4513 - ETA: 8s - loss: 8.8584 - acc: 0.450 - ETA: 7s - loss: 8.8618 - acc: 0.450 - ETA: 6s - loss: 8.8448 - acc: 0.451 - ETA: 4s - loss: 8.8001 - acc: 0.454 - ETA: 3s - loss: 8.7913 - acc: 0.454 - ETA: 2s - loss: 8.7561 - acc: 0.456 - ETA: 1s - loss: 8.7408 - acc: 0.457 - ETA: 0s - loss: 8.7390 - acc: 0.457 - 40s 10ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3882/3882 [==============================] - ETA: 35s - loss: 9.3183 - acc: 0.42 - ETA: 34s - loss: 9.0664 - acc: 0.43 - ETA: 33s - loss: 9.4442 - acc: 0.41 - ETA: 32s - loss: 9.5701 - acc: 0.40 - ETA: 30s - loss: 9.5198 - acc: 0.40 - ETA: 29s - loss: 9.2133 - acc: 0.42 - ETA: 28s - loss: 9.2103 - acc: 0.42 - ETA: 27s - loss: 9.0350 - acc: 0.43 - ETA: 25s - loss: 9.1224 - acc: 0.43 - ETA: 24s - loss: 9.1420 - acc: 0.43 - ETA: 23s - loss: 9.0206 - acc: 0.44 - ETA: 22s - loss: 8.9510 - acc: 0.44 - ETA: 21s - loss: 8.9018 - acc: 0.44 - ETA: 19s - loss: 8.8865 - acc: 0.44 - ETA: 18s - loss: 8.9489 - acc: 0.44 - ETA: 17s - loss: 8.9326 - acc: 0.44 - ETA: 16s - loss: 8.9331 - acc: 0.44 - ETA: 15s - loss: 8.9125 - acc: 0.44 - ETA: 13s - loss: 8.8676 - acc: 0.44 - ETA: 12s - loss: 8.9216 - acc: 0.44 - ETA: 11s - loss: 8.9405 - acc: 0.44 - ETA: 10s - loss: 8.9348 - acc: 0.44 - ETA: 8s - loss: 8.9022 - acc: 0.4477 - ETA: 7s - loss: 8.8461 - acc: 0.451 - ETA: 6s - loss: 8.8196 - acc: 0.452 - ETA: 5s - loss: 8.7565 - acc: 0.456 - ETA: 4s - loss: 8.7633 - acc: 0.456 - ETA: 2s - loss: 8.7741 - acc: 0.455 - ETA: 1s - loss: 8.7712 - acc: 0.455 - ETA: 0s - loss: 8.7642 - acc: 0.456 - 42s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 14/50\n",
      "3882/3882 [==============================] - ETA: 37s - loss: 8.0590 - acc: 0.50 - ETA: 35s - loss: 8.2479 - acc: 0.48 - ETA: 33s - loss: 8.7306 - acc: 0.45 - ETA: 32s - loss: 8.6572 - acc: 0.46 - ETA: 31s - loss: 8.5124 - acc: 0.47 - ETA: 29s - loss: 8.5627 - acc: 0.46 - ETA: 28s - loss: 8.5987 - acc: 0.46 - ETA: 26s - loss: 8.6257 - acc: 0.46 - ETA: 25s - loss: 8.7166 - acc: 0.45 - ETA: 24s - loss: 8.8020 - acc: 0.45 - ETA: 23s - loss: 8.7688 - acc: 0.45 - ETA: 22s - loss: 8.7516 - acc: 0.45 - ETA: 21s - loss: 8.6209 - acc: 0.46 - ETA: 19s - loss: 8.5448 - acc: 0.46 - ETA: 18s - loss: 8.5376 - acc: 0.47 - ETA: 17s - loss: 8.6100 - acc: 0.46 - ETA: 16s - loss: 8.5998 - acc: 0.46 - ETA: 14s - loss: 8.6047 - acc: 0.46 - ETA: 13s - loss: 8.5959 - acc: 0.46 - ETA: 12s - loss: 8.6446 - acc: 0.46 - ETA: 11s - loss: 8.6647 - acc: 0.46 - ETA: 10s - loss: 8.6486 - acc: 0.46 - ETA: 8s - loss: 8.6558 - acc: 0.4630 - ETA: 7s - loss: 8.6992 - acc: 0.460 - ETA: 6s - loss: 8.6685 - acc: 0.462 - ETA: 5s - loss: 8.7032 - acc: 0.460 - ETA: 4s - loss: 8.7446 - acc: 0.457 - ETA: 2s - loss: 8.7516 - acc: 0.457 - ETA: 1s - loss: 8.7755 - acc: 0.455 - ETA: 0s - loss: 8.7432 - acc: 0.457 - 42s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 15/50\n",
      "3882/3882 [==============================] - ETA: 35s - loss: 9.0664 - acc: 0.43 - ETA: 33s - loss: 9.1294 - acc: 0.43 - ETA: 33s - loss: 9.0664 - acc: 0.43 - ETA: 31s - loss: 8.9720 - acc: 0.44 - ETA: 30s - loss: 8.8146 - acc: 0.45 - ETA: 29s - loss: 8.9195 - acc: 0.44 - ETA: 28s - loss: 8.8865 - acc: 0.44 - ETA: 27s - loss: 8.8775 - acc: 0.44 - ETA: 26s - loss: 8.9545 - acc: 0.44 - ETA: 24s - loss: 8.8524 - acc: 0.45 - ETA: 23s - loss: 8.7688 - acc: 0.45 - ETA: 22s - loss: 8.7831 - acc: 0.45 - ETA: 21s - loss: 8.7952 - acc: 0.45 - ETA: 20s - loss: 8.8775 - acc: 0.44 - ETA: 18s - loss: 8.8817 - acc: 0.44 - ETA: 17s - loss: 8.8933 - acc: 0.44 - ETA: 16s - loss: 8.8664 - acc: 0.44 - ETA: 15s - loss: 8.7936 - acc: 0.45 - ETA: 13s - loss: 8.7814 - acc: 0.45 - ETA: 12s - loss: 8.7705 - acc: 0.45 - ETA: 11s - loss: 8.7726 - acc: 0.45 - ETA: 10s - loss: 8.7402 - acc: 0.45 - ETA: 8s - loss: 8.7270 - acc: 0.4586 - ETA: 7s - loss: 8.7149 - acc: 0.459 - ETA: 6s - loss: 8.7390 - acc: 0.457 - ETA: 5s - loss: 8.7129 - acc: 0.459 - ETA: 4s - loss: 8.7493 - acc: 0.457 - ETA: 2s - loss: 8.7291 - acc: 0.458 - ETA: 1s - loss: 8.7060 - acc: 0.459 - ETA: 0s - loss: 8.7432 - acc: 0.457 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 16/50\n",
      "3882/3882 [==============================] - ETA: 36s - loss: 8.1850 - acc: 0.49 - ETA: 35s - loss: 8.3739 - acc: 0.48 - ETA: 33s - loss: 8.4368 - acc: 0.47 - ETA: 31s - loss: 8.8146 - acc: 0.45 - ETA: 30s - loss: 8.8650 - acc: 0.45 - ETA: 29s - loss: 8.8775 - acc: 0.44 - ETA: 28s - loss: 8.7606 - acc: 0.45 - ETA: 26s - loss: 8.7359 - acc: 0.45 - ETA: 25s - loss: 8.5767 - acc: 0.46 - ETA: 24s - loss: 8.6257 - acc: 0.46 - ETA: 23s - loss: 8.6658 - acc: 0.46 - ETA: 22s - loss: 8.6677 - acc: 0.46 - ETA: 21s - loss: 8.6209 - acc: 0.46 - ETA: 19s - loss: 8.5987 - acc: 0.46 - ETA: 18s - loss: 8.5543 - acc: 0.46 - ETA: 17s - loss: 8.5864 - acc: 0.46 - ETA: 16s - loss: 8.6072 - acc: 0.46 - ETA: 15s - loss: 8.6117 - acc: 0.46 - ETA: 13s - loss: 8.6290 - acc: 0.46 - ETA: 12s - loss: 8.6761 - acc: 0.46 - ETA: 11s - loss: 8.5747 - acc: 0.46 - ETA: 10s - loss: 8.5227 - acc: 0.47 - ETA: 8s - loss: 8.5463 - acc: 0.4698 - ETA: 7s - loss: 8.5575 - acc: 0.469 - ETA: 6s - loss: 8.6131 - acc: 0.465 - ETA: 5s - loss: 8.6451 - acc: 0.463 - ETA: 4s - loss: 8.6467 - acc: 0.463 - ETA: 2s - loss: 8.6887 - acc: 0.460 - ETA: 1s - loss: 8.7060 - acc: 0.459 - ETA: 0s - loss: 8.7306 - acc: 0.458 - 42s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 17/50\n",
      "3882/3882 [==============================] - ETA: 36s - loss: 8.6887 - acc: 0.46 - ETA: 34s - loss: 9.0664 - acc: 0.43 - ETA: 33s - loss: 8.5627 - acc: 0.46 - ETA: 32s - loss: 8.6887 - acc: 0.46 - ETA: 31s - loss: 8.6887 - acc: 0.46 - ETA: 29s - loss: 8.6887 - acc: 0.46 - ETA: 28s - loss: 8.6527 - acc: 0.46 - ETA: 27s - loss: 8.6257 - acc: 0.46 - ETA: 26s - loss: 8.6607 - acc: 0.46 - ETA: 25s - loss: 8.6257 - acc: 0.46 - ETA: 23s - loss: 8.5856 - acc: 0.46 - ETA: 22s - loss: 8.7097 - acc: 0.45 - ETA: 21s - loss: 8.7177 - acc: 0.45 - ETA: 20s - loss: 8.7156 - acc: 0.45 - ETA: 18s - loss: 8.7138 - acc: 0.45 - ETA: 17s - loss: 8.7752 - acc: 0.45 - ETA: 16s - loss: 8.8294 - acc: 0.45 - ETA: 15s - loss: 8.7796 - acc: 0.45 - ETA: 13s - loss: 8.7947 - acc: 0.45 - ETA: 12s - loss: 8.7327 - acc: 0.45 - ETA: 11s - loss: 8.7366 - acc: 0.45 - ETA: 10s - loss: 8.7173 - acc: 0.45 - ETA: 8s - loss: 8.7817 - acc: 0.4552 - ETA: 7s - loss: 8.8251 - acc: 0.452 - ETA: 6s - loss: 8.8045 - acc: 0.453 - ETA: 5s - loss: 8.8097 - acc: 0.453 - ETA: 4s - loss: 8.8432 - acc: 0.451 - ETA: 2s - loss: 8.8062 - acc: 0.453 - ETA: 1s - loss: 8.7891 - acc: 0.454 - ETA: 0s - loss: 8.7354 - acc: 0.457 - 42s 11ms/step - loss: 8.7447 - acc: 0.4572 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 18/50\n",
      "3882/3882 [==============================] - ETA: 36s - loss: 9.0664 - acc: 0.43 - ETA: 35s - loss: 9.3812 - acc: 0.41 - ETA: 33s - loss: 9.1084 - acc: 0.43 - ETA: 32s - loss: 9.2238 - acc: 0.42 - ETA: 31s - loss: 9.0664 - acc: 0.43 - ETA: 30s - loss: 9.1084 - acc: 0.43 - ETA: 28s - loss: 8.9585 - acc: 0.44 - ETA: 27s - loss: 8.9562 - acc: 0.44 - ETA: 26s - loss: 8.9265 - acc: 0.44 - ETA: 24s - loss: 8.8398 - acc: 0.45 - ETA: 23s - loss: 8.7573 - acc: 0.45 - ETA: 22s - loss: 8.7726 - acc: 0.45 - ETA: 21s - loss: 8.7371 - acc: 0.45 - ETA: 20s - loss: 8.8146 - acc: 0.45 - ETA: 18s - loss: 8.8566 - acc: 0.45 - ETA: 17s - loss: 8.8461 - acc: 0.45 - ETA: 16s - loss: 8.7924 - acc: 0.45 - ETA: 15s - loss: 8.8426 - acc: 0.45 - ETA: 13s - loss: 8.8477 - acc: 0.45 - ETA: 12s - loss: 8.7642 - acc: 0.45 - ETA: 11s - loss: 8.7306 - acc: 0.45 - ETA: 10s - loss: 8.7516 - acc: 0.45 - ETA: 8s - loss: 8.7325 - acc: 0.4582 - ETA: 7s - loss: 8.7411 - acc: 0.457 - ETA: 6s - loss: 8.7743 - acc: 0.455 - ETA: 5s - loss: 8.7807 - acc: 0.455 - ETA: 4s - loss: 8.7446 - acc: 0.457 - ETA: 2s - loss: 8.7336 - acc: 0.458 - ETA: 1s - loss: 8.7408 - acc: 0.457 - ETA: 0s - loss: 8.7390 - acc: 0.457 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3882/3882 [==============================] - ETA: 36s - loss: 9.0664 - acc: 0.43 - ETA: 36s - loss: 8.8775 - acc: 0.44 - ETA: 34s - loss: 8.8985 - acc: 0.44 - ETA: 32s - loss: 9.1609 - acc: 0.43 - ETA: 31s - loss: 9.2175 - acc: 0.42 - ETA: 30s - loss: 9.0664 - acc: 0.43 - ETA: 29s - loss: 8.9945 - acc: 0.44 - ETA: 27s - loss: 8.8146 - acc: 0.45 - ETA: 26s - loss: 8.6187 - acc: 0.46 - ETA: 25s - loss: 8.6509 - acc: 0.46 - ETA: 23s - loss: 8.5513 - acc: 0.46 - ETA: 22s - loss: 8.6362 - acc: 0.46 - ETA: 21s - loss: 8.6790 - acc: 0.46 - ETA: 20s - loss: 8.6437 - acc: 0.46 - ETA: 18s - loss: 8.6971 - acc: 0.46 - ETA: 17s - loss: 8.7123 - acc: 0.45 - ETA: 16s - loss: 8.6887 - acc: 0.46 - ETA: 15s - loss: 8.7376 - acc: 0.45 - ETA: 13s - loss: 8.7019 - acc: 0.46 - ETA: 12s - loss: 8.7264 - acc: 0.45 - ETA: 11s - loss: 8.7186 - acc: 0.45 - ETA: 10s - loss: 8.7345 - acc: 0.45 - ETA: 9s - loss: 8.7817 - acc: 0.4552 - ETA: 7s - loss: 8.8041 - acc: 0.453 - ETA: 6s - loss: 8.8146 - acc: 0.453 - ETA: 5s - loss: 8.8582 - acc: 0.450 - ETA: 4s - loss: 8.8286 - acc: 0.452 - ETA: 2s - loss: 8.8506 - acc: 0.450 - ETA: 1s - loss: 8.7885 - acc: 0.454 - ETA: 0s - loss: 8.7684 - acc: 0.456 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 20/50\n",
      "3882/3882 [==============================] - ETA: 37s - loss: 8.4368 - acc: 0.47 - ETA: 36s - loss: 8.3109 - acc: 0.48 - ETA: 34s - loss: 8.5627 - acc: 0.46 - ETA: 32s - loss: 8.5627 - acc: 0.46 - ETA: 31s - loss: 8.5376 - acc: 0.47 - ETA: 30s - loss: 8.4998 - acc: 0.47 - ETA: 29s - loss: 8.5987 - acc: 0.46 - ETA: 27s - loss: 8.5785 - acc: 0.46 - ETA: 26s - loss: 8.6047 - acc: 0.46 - ETA: 25s - loss: 8.4242 - acc: 0.47 - ETA: 23s - loss: 8.5398 - acc: 0.47 - ETA: 22s - loss: 8.6992 - acc: 0.46 - ETA: 21s - loss: 8.7080 - acc: 0.45 - ETA: 20s - loss: 8.7156 - acc: 0.45 - ETA: 18s - loss: 8.7138 - acc: 0.45 - ETA: 17s - loss: 8.7438 - acc: 0.45 - ETA: 16s - loss: 8.7331 - acc: 0.45 - ETA: 15s - loss: 8.6537 - acc: 0.46 - ETA: 13s - loss: 8.6423 - acc: 0.46 - ETA: 12s - loss: 8.6950 - acc: 0.46 - ETA: 11s - loss: 8.7007 - acc: 0.46 - ETA: 10s - loss: 8.6257 - acc: 0.46 - ETA: 9s - loss: 8.6668 - acc: 0.4623 - ETA: 7s - loss: 8.6729 - acc: 0.461 - ETA: 6s - loss: 8.7239 - acc: 0.458 - ETA: 5s - loss: 8.7080 - acc: 0.459 - ETA: 4s - loss: 8.7260 - acc: 0.458 - ETA: 2s - loss: 8.7336 - acc: 0.458 - ETA: 1s - loss: 8.7364 - acc: 0.458 - ETA: 0s - loss: 8.7558 - acc: 0.456 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 21/50\n",
      "3882/3882 [==============================] - ETA: 36s - loss: 8.1850 - acc: 0.49 - ETA: 34s - loss: 8.1220 - acc: 0.49 - ETA: 34s - loss: 8.2269 - acc: 0.48 - ETA: 32s - loss: 8.2165 - acc: 0.49 - ETA: 31s - loss: 8.4116 - acc: 0.47 - ETA: 30s - loss: 8.3529 - acc: 0.48 - ETA: 29s - loss: 8.4008 - acc: 0.47 - ETA: 27s - loss: 8.3581 - acc: 0.48 - ETA: 26s - loss: 8.4648 - acc: 0.47 - ETA: 25s - loss: 8.4368 - acc: 0.47 - ETA: 24s - loss: 8.5971 - acc: 0.46 - ETA: 22s - loss: 8.5732 - acc: 0.46 - ETA: 21s - loss: 8.6499 - acc: 0.46 - ETA: 20s - loss: 8.6707 - acc: 0.46 - ETA: 19s - loss: 8.6551 - acc: 0.46 - ETA: 17s - loss: 8.6808 - acc: 0.46 - ETA: 16s - loss: 8.6738 - acc: 0.46 - ETA: 15s - loss: 8.7306 - acc: 0.45 - ETA: 14s - loss: 8.8013 - acc: 0.45 - ETA: 12s - loss: 8.7705 - acc: 0.45 - ETA: 11s - loss: 8.7366 - acc: 0.45 - ETA: 10s - loss: 8.7802 - acc: 0.45 - ETA: 9s - loss: 8.8146 - acc: 0.4531 - ETA: 7s - loss: 8.8513 - acc: 0.450 - ETA: 6s - loss: 8.8247 - acc: 0.452 - ETA: 5s - loss: 8.7371 - acc: 0.457 - ETA: 4s - loss: 8.7400 - acc: 0.457 - ETA: 2s - loss: 8.7381 - acc: 0.457 - ETA: 1s - loss: 8.7277 - acc: 0.458 - ETA: 0s - loss: 8.7390 - acc: 0.457 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 22/50\n",
      "3882/3882 [==============================] - ETA: 36s - loss: 8.3109 - acc: 0.48 - ETA: 34s - loss: 7.6813 - acc: 0.52 - ETA: 33s - loss: 7.8492 - acc: 0.51 - ETA: 31s - loss: 8.1535 - acc: 0.49 - ETA: 30s - loss: 8.4368 - acc: 0.47 - ETA: 29s - loss: 8.5208 - acc: 0.47 - ETA: 28s - loss: 8.3829 - acc: 0.47 - ETA: 27s - loss: 8.4211 - acc: 0.47 - ETA: 25s - loss: 8.4648 - acc: 0.47 - ETA: 24s - loss: 8.5627 - acc: 0.46 - ETA: 23s - loss: 8.6543 - acc: 0.46 - ETA: 22s - loss: 8.7306 - acc: 0.45 - ETA: 21s - loss: 8.7662 - acc: 0.45 - ETA: 20s - loss: 8.8416 - acc: 0.45 - ETA: 18s - loss: 8.7810 - acc: 0.45 - ETA: 17s - loss: 8.8067 - acc: 0.45 - ETA: 16s - loss: 8.8368 - acc: 0.45 - ETA: 15s - loss: 8.7936 - acc: 0.45 - ETA: 13s - loss: 8.7218 - acc: 0.45 - ETA: 12s - loss: 8.7264 - acc: 0.45 - ETA: 11s - loss: 8.7246 - acc: 0.45 - ETA: 10s - loss: 8.7345 - acc: 0.45 - ETA: 9s - loss: 8.7160 - acc: 0.4592 - ETA: 7s - loss: 8.6939 - acc: 0.460 - ETA: 6s - loss: 8.7189 - acc: 0.459 - ETA: 5s - loss: 8.7129 - acc: 0.459 - ETA: 4s - loss: 8.7773 - acc: 0.455 - ETA: 2s - loss: 8.7561 - acc: 0.456 - ETA: 1s - loss: 8.7668 - acc: 0.456 - ETA: 0s - loss: 8.7390 - acc: 0.457 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 23/50\n",
      "3882/3882 [==============================] - ETA: 35s - loss: 8.0590 - acc: 0.50 - ETA: 34s - loss: 8.3109 - acc: 0.48 - ETA: 33s - loss: 8.4788 - acc: 0.47 - ETA: 31s - loss: 8.8461 - acc: 0.45 - ETA: 30s - loss: 8.8398 - acc: 0.45 - ETA: 29s - loss: 8.8146 - acc: 0.45 - ETA: 28s - loss: 8.7786 - acc: 0.45 - ETA: 26s - loss: 8.7359 - acc: 0.45 - ETA: 25s - loss: 8.7446 - acc: 0.45 - ETA: 24s - loss: 8.7642 - acc: 0.45 - ETA: 23s - loss: 8.7688 - acc: 0.45 - ETA: 22s - loss: 8.7936 - acc: 0.45 - ETA: 21s - loss: 8.7565 - acc: 0.45 - ETA: 19s - loss: 8.7516 - acc: 0.45 - ETA: 18s - loss: 8.8230 - acc: 0.45 - ETA: 17s - loss: 8.7674 - acc: 0.45 - ETA: 16s - loss: 8.7479 - acc: 0.45 - ETA: 15s - loss: 8.7446 - acc: 0.45 - ETA: 13s - loss: 8.7616 - acc: 0.45 - ETA: 12s - loss: 8.7390 - acc: 0.45 - ETA: 11s - loss: 8.6767 - acc: 0.46 - ETA: 10s - loss: 8.7058 - acc: 0.45 - ETA: 8s - loss: 8.6722 - acc: 0.4620 - ETA: 7s - loss: 8.6729 - acc: 0.461 - ETA: 6s - loss: 8.6786 - acc: 0.461 - ETA: 5s - loss: 8.6741 - acc: 0.461 - ETA: 4s - loss: 8.7073 - acc: 0.459 - ETA: 2s - loss: 8.7156 - acc: 0.459 - ETA: 1s - loss: 8.6887 - acc: 0.460 - ETA: 0s - loss: 8.7432 - acc: 0.457 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 24/50\n",
      "3882/3882 [==============================] - ETA: 36s - loss: 7.5554 - acc: 0.53 - ETA: 35s - loss: 8.8146 - acc: 0.45 - ETA: 34s - loss: 8.6887 - acc: 0.46 - ETA: 33s - loss: 8.5942 - acc: 0.46 - ETA: 31s - loss: 8.6383 - acc: 0.46 - ETA: 30s - loss: 8.6257 - acc: 0.46 - ETA: 28s - loss: 8.7966 - acc: 0.45 - ETA: 27s - loss: 8.8775 - acc: 0.44 - ETA: 26s - loss: 8.8566 - acc: 0.45 - ETA: 25s - loss: 8.8524 - acc: 0.45 - ETA: 23s - loss: 8.7802 - acc: 0.45 - ETA: 22s - loss: 8.8041 - acc: 0.45 - ETA: 21s - loss: 8.8243 - acc: 0.45 - ETA: 20s - loss: 8.8056 - acc: 0.45 - ETA: 18s - loss: 8.8398 - acc: 0.45 - ETA: 17s - loss: 8.8303 - acc: 0.45 - ETA: 16s - loss: 8.8738 - acc: 0.44 - ETA: 15s - loss: 8.8426 - acc: 0.45 - ETA: 13s - loss: 8.8146 - acc: 0.45 - ETA: 12s - loss: 8.8020 - acc: 0.45 - ETA: 11s - loss: 8.8026 - acc: 0.45 - ETA: 10s - loss: 8.8031 - acc: 0.45 - ETA: 8s - loss: 8.7763 - acc: 0.4555 - ETA: 7s - loss: 8.7464 - acc: 0.457 - ETA: 6s - loss: 8.7390 - acc: 0.457 - ETA: 5s - loss: 8.6983 - acc: 0.460 - ETA: 4s - loss: 8.6793 - acc: 0.461 - ETA: 2s - loss: 8.7156 - acc: 0.459 - ETA: 1s - loss: 8.7625 - acc: 0.456 - ETA: 0s - loss: 8.7558 - acc: 0.456 - 42s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3882/3882 [==============================] - ETA: 37s - loss: 9.9479 - acc: 0.38 - ETA: 35s - loss: 8.8775 - acc: 0.44 - ETA: 33s - loss: 8.8566 - acc: 0.45 - ETA: 32s - loss: 9.0664 - acc: 0.43 - ETA: 30s - loss: 8.8650 - acc: 0.45 - ETA: 29s - loss: 8.8146 - acc: 0.45 - ETA: 28s - loss: 8.7606 - acc: 0.45 - ETA: 27s - loss: 8.8933 - acc: 0.44 - ETA: 25s - loss: 8.8006 - acc: 0.45 - ETA: 24s - loss: 8.7390 - acc: 0.45 - ETA: 23s - loss: 8.7230 - acc: 0.45 - ETA: 22s - loss: 8.7306 - acc: 0.45 - ETA: 21s - loss: 8.7855 - acc: 0.45 - ETA: 19s - loss: 8.7966 - acc: 0.45 - ETA: 18s - loss: 8.6383 - acc: 0.46 - ETA: 17s - loss: 8.7044 - acc: 0.46 - ETA: 16s - loss: 8.6590 - acc: 0.46 - ETA: 14s - loss: 8.6887 - acc: 0.46 - ETA: 13s - loss: 8.6688 - acc: 0.46 - ETA: 12s - loss: 8.7013 - acc: 0.46 - ETA: 11s - loss: 8.7067 - acc: 0.45 - ETA: 10s - loss: 8.7001 - acc: 0.46 - ETA: 8s - loss: 8.6558 - acc: 0.4630 - ETA: 7s - loss: 8.6782 - acc: 0.461 - ETA: 6s - loss: 8.6635 - acc: 0.462 - ETA: 5s - loss: 8.6548 - acc: 0.463 - ETA: 4s - loss: 8.6933 - acc: 0.460 - ETA: 2s - loss: 8.7156 - acc: 0.459 - ETA: 1s - loss: 8.7191 - acc: 0.459 - ETA: 0s - loss: 8.7306 - acc: 0.458 - 42s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 26/50\n",
      "3882/3882 [==============================] - ETA: 35s - loss: 7.6813 - acc: 0.52 - ETA: 34s - loss: 8.3109 - acc: 0.48 - ETA: 33s - loss: 8.2689 - acc: 0.48 - ETA: 32s - loss: 8.4998 - acc: 0.47 - ETA: 30s - loss: 8.5879 - acc: 0.46 - ETA: 29s - loss: 8.4788 - acc: 0.47 - ETA: 28s - loss: 8.7246 - acc: 0.45 - ETA: 27s - loss: 8.9405 - acc: 0.44 - ETA: 26s - loss: 8.7446 - acc: 0.45 - ETA: 24s - loss: 8.7264 - acc: 0.45 - ETA: 23s - loss: 8.6887 - acc: 0.46 - ETA: 22s - loss: 8.7201 - acc: 0.45 - ETA: 21s - loss: 8.8049 - acc: 0.45 - ETA: 19s - loss: 8.7966 - acc: 0.45 - ETA: 18s - loss: 8.7894 - acc: 0.45 - ETA: 17s - loss: 8.7910 - acc: 0.45 - ETA: 16s - loss: 8.7998 - acc: 0.45 - ETA: 15s - loss: 8.7446 - acc: 0.45 - ETA: 13s - loss: 8.7417 - acc: 0.45 - ETA: 12s - loss: 8.7201 - acc: 0.45 - ETA: 11s - loss: 8.6647 - acc: 0.46 - ETA: 10s - loss: 8.7116 - acc: 0.45 - ETA: 8s - loss: 8.6996 - acc: 0.4603 - ETA: 7s - loss: 8.6834 - acc: 0.461 - ETA: 6s - loss: 8.6736 - acc: 0.461 - ETA: 5s - loss: 8.7129 - acc: 0.459 - ETA: 4s - loss: 8.7260 - acc: 0.458 - ETA: 2s - loss: 8.7471 - acc: 0.457 - ETA: 1s - loss: 8.7495 - acc: 0.457 - ETA: 0s - loss: 8.7600 - acc: 0.456 - 42s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 27/50\n",
      "3882/3882 [==============================] - ETA: 34s - loss: 8.9405 - acc: 0.44 - ETA: 32s - loss: 8.6887 - acc: 0.46 - ETA: 30s - loss: 8.7726 - acc: 0.45 - ETA: 30s - loss: 8.8461 - acc: 0.45 - ETA: 29s - loss: 8.9657 - acc: 0.44 - ETA: 28s - loss: 9.1084 - acc: 0.43 - ETA: 27s - loss: 9.0844 - acc: 0.43 - ETA: 26s - loss: 9.0350 - acc: 0.43 - ETA: 25s - loss: 9.0105 - acc: 0.44 - ETA: 24s - loss: 8.9783 - acc: 0.44 - ETA: 23s - loss: 8.9749 - acc: 0.44 - ETA: 22s - loss: 8.9405 - acc: 0.44 - ETA: 21s - loss: 8.9308 - acc: 0.44 - ETA: 19s - loss: 8.9675 - acc: 0.44 - ETA: 18s - loss: 8.8482 - acc: 0.45 - ETA: 17s - loss: 8.8382 - acc: 0.45 - ETA: 16s - loss: 8.8664 - acc: 0.44 - ETA: 15s - loss: 8.8426 - acc: 0.45 - ETA: 13s - loss: 8.8013 - acc: 0.45 - ETA: 12s - loss: 8.7327 - acc: 0.45 - ETA: 11s - loss: 8.7546 - acc: 0.45 - ETA: 10s - loss: 8.7116 - acc: 0.45 - ETA: 8s - loss: 8.7051 - acc: 0.4599 - ETA: 7s - loss: 8.7149 - acc: 0.459 - ETA: 6s - loss: 8.6987 - acc: 0.460 - ETA: 5s - loss: 8.6741 - acc: 0.461 - ETA: 4s - loss: 8.6887 - acc: 0.460 - ETA: 2s - loss: 8.6752 - acc: 0.461 - ETA: 1s - loss: 8.6800 - acc: 0.461 - ETA: 0s - loss: 8.7222 - acc: 0.458 - 42s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 28/50\n",
      "3882/3882 [==============================] - ETA: 36s - loss: 7.5554 - acc: 0.53 - ETA: 35s - loss: 7.7442 - acc: 0.51 - ETA: 34s - loss: 8.4788 - acc: 0.47 - ETA: 33s - loss: 8.4683 - acc: 0.47 - ETA: 31s - loss: 8.3864 - acc: 0.47 - ETA: 30s - loss: 8.4368 - acc: 0.47 - ETA: 28s - loss: 8.4548 - acc: 0.47 - ETA: 27s - loss: 8.4211 - acc: 0.47 - ETA: 26s - loss: 8.6047 - acc: 0.46 - ETA: 25s - loss: 8.5501 - acc: 0.46 - ETA: 24s - loss: 8.6429 - acc: 0.46 - ETA: 22s - loss: 8.6467 - acc: 0.46 - ETA: 21s - loss: 8.6402 - acc: 0.46 - ETA: 20s - loss: 8.6257 - acc: 0.46 - ETA: 19s - loss: 8.6803 - acc: 0.46 - ETA: 17s - loss: 8.7123 - acc: 0.45 - ETA: 16s - loss: 8.6887 - acc: 0.46 - ETA: 15s - loss: 8.6677 - acc: 0.46 - ETA: 14s - loss: 8.6622 - acc: 0.46 - ETA: 12s - loss: 8.6761 - acc: 0.46 - ETA: 11s - loss: 8.6647 - acc: 0.46 - ETA: 10s - loss: 8.6829 - acc: 0.46 - ETA: 9s - loss: 8.6832 - acc: 0.4613 - ETA: 7s - loss: 8.7201 - acc: 0.459 - ETA: 6s - loss: 8.7340 - acc: 0.458 - ETA: 5s - loss: 8.7419 - acc: 0.457 - ETA: 4s - loss: 8.7866 - acc: 0.454 - ETA: 2s - loss: 8.7381 - acc: 0.457 - ETA: 1s - loss: 8.7147 - acc: 0.459 - ETA: 0s - loss: 8.7558 - acc: 0.456 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 29/50\n",
      "3882/3882 [==============================] - ETA: 35s - loss: 9.1924 - acc: 0.42 - ETA: 35s - loss: 8.2479 - acc: 0.48 - ETA: 33s - loss: 8.6467 - acc: 0.46 - ETA: 32s - loss: 8.8461 - acc: 0.45 - ETA: 31s - loss: 8.8146 - acc: 0.45 - ETA: 29s - loss: 8.6677 - acc: 0.46 - ETA: 28s - loss: 8.7786 - acc: 0.45 - ETA: 27s - loss: 8.7988 - acc: 0.45 - ETA: 26s - loss: 8.7306 - acc: 0.45 - ETA: 25s - loss: 8.7264 - acc: 0.45 - ETA: 23s - loss: 8.7573 - acc: 0.45 - ETA: 22s - loss: 8.8041 - acc: 0.45 - ETA: 21s - loss: 8.8243 - acc: 0.45 - ETA: 20s - loss: 8.8506 - acc: 0.45 - ETA: 19s - loss: 8.8901 - acc: 0.44 - ETA: 17s - loss: 8.8775 - acc: 0.44 - ETA: 16s - loss: 8.8738 - acc: 0.44 - ETA: 15s - loss: 8.8356 - acc: 0.45 - ETA: 14s - loss: 8.7947 - acc: 0.45 - ETA: 12s - loss: 8.8083 - acc: 0.45 - ETA: 11s - loss: 8.7966 - acc: 0.45 - ETA: 10s - loss: 8.7802 - acc: 0.45 - ETA: 9s - loss: 8.7653 - acc: 0.4562 - ETA: 7s - loss: 8.8093 - acc: 0.453 - ETA: 6s - loss: 8.7693 - acc: 0.455 - ETA: 5s - loss: 8.7613 - acc: 0.456 - ETA: 4s - loss: 8.7586 - acc: 0.456 - ETA: 2s - loss: 8.8056 - acc: 0.453 - ETA: 1s - loss: 8.7755 - acc: 0.455 - ETA: 0s - loss: 8.7642 - acc: 0.456 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 30/50\n",
      "3882/3882 [==============================] - ETA: 36s - loss: 9.5701 - acc: 0.40 - ETA: 35s - loss: 9.5072 - acc: 0.41 - ETA: 33s - loss: 9.0245 - acc: 0.44 - ETA: 32s - loss: 8.9405 - acc: 0.44 - ETA: 31s - loss: 8.7642 - acc: 0.45 - ETA: 29s - loss: 8.7726 - acc: 0.45 - ETA: 28s - loss: 8.7067 - acc: 0.45 - ETA: 27s - loss: 8.6257 - acc: 0.46 - ETA: 26s - loss: 8.5767 - acc: 0.46 - ETA: 24s - loss: 8.6131 - acc: 0.46 - ETA: 23s - loss: 8.5742 - acc: 0.46 - ETA: 22s - loss: 8.6152 - acc: 0.46 - ETA: 21s - loss: 8.5821 - acc: 0.46 - ETA: 19s - loss: 8.6887 - acc: 0.46 - ETA: 18s - loss: 8.7055 - acc: 0.45 - ETA: 17s - loss: 8.6178 - acc: 0.46 - ETA: 16s - loss: 8.6146 - acc: 0.46 - ETA: 15s - loss: 8.6257 - acc: 0.46 - ETA: 13s - loss: 8.7152 - acc: 0.45 - ETA: 12s - loss: 8.6950 - acc: 0.46 - ETA: 11s - loss: 8.7486 - acc: 0.45 - ETA: 10s - loss: 8.7173 - acc: 0.45 - ETA: 8s - loss: 8.7653 - acc: 0.4562 - ETA: 7s - loss: 8.7411 - acc: 0.457 - ETA: 6s - loss: 8.7541 - acc: 0.456 - ETA: 5s - loss: 8.7468 - acc: 0.457 - ETA: 4s - loss: 8.7586 - acc: 0.456 - ETA: 2s - loss: 8.7381 - acc: 0.457 - ETA: 1s - loss: 8.7408 - acc: 0.457 - ETA: 0s - loss: 8.7558 - acc: 0.456 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3882/3882 [==============================] - ETA: 35s - loss: 9.9479 - acc: 0.38 - ETA: 35s - loss: 9.6331 - acc: 0.40 - ETA: 34s - loss: 9.4022 - acc: 0.41 - ETA: 32s - loss: 9.0664 - acc: 0.43 - ETA: 31s - loss: 9.1168 - acc: 0.43 - ETA: 30s - loss: 9.1504 - acc: 0.43 - ETA: 28s - loss: 9.1204 - acc: 0.43 - ETA: 27s - loss: 9.0664 - acc: 0.43 - ETA: 26s - loss: 8.9965 - acc: 0.44 - ETA: 25s - loss: 8.9783 - acc: 0.44 - ETA: 23s - loss: 8.8375 - acc: 0.45 - ETA: 22s - loss: 8.8251 - acc: 0.45 - ETA: 21s - loss: 8.8146 - acc: 0.45 - ETA: 20s - loss: 8.8326 - acc: 0.45 - ETA: 18s - loss: 8.7978 - acc: 0.45 - ETA: 17s - loss: 8.7359 - acc: 0.45 - ETA: 16s - loss: 8.8220 - acc: 0.45 - ETA: 15s - loss: 8.7097 - acc: 0.45 - ETA: 13s - loss: 8.6887 - acc: 0.46 - ETA: 12s - loss: 8.6824 - acc: 0.46 - ETA: 11s - loss: 8.7067 - acc: 0.45 - ETA: 10s - loss: 8.7802 - acc: 0.45 - ETA: 8s - loss: 8.8201 - acc: 0.4528 - ETA: 7s - loss: 8.8093 - acc: 0.453 - ETA: 6s - loss: 8.8196 - acc: 0.452 - ETA: 5s - loss: 8.7952 - acc: 0.454 - ETA: 4s - loss: 8.7959 - acc: 0.454 - ETA: 2s - loss: 8.7741 - acc: 0.455 - ETA: 1s - loss: 8.7581 - acc: 0.456 - ETA: 0s - loss: 8.7474 - acc: 0.457 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 32/50\n",
      "3882/3882 [==============================] - ETA: 37s - loss: 8.4368 - acc: 0.47 - ETA: 34s - loss: 8.8146 - acc: 0.45 - ETA: 32s - loss: 8.8146 - acc: 0.45 - ETA: 31s - loss: 8.9405 - acc: 0.44 - ETA: 30s - loss: 8.9657 - acc: 0.44 - ETA: 29s - loss: 9.0245 - acc: 0.44 - ETA: 28s - loss: 8.9405 - acc: 0.44 - ETA: 27s - loss: 8.8146 - acc: 0.45 - ETA: 25s - loss: 8.8006 - acc: 0.45 - ETA: 24s - loss: 8.7264 - acc: 0.45 - ETA: 23s - loss: 8.7688 - acc: 0.45 - ETA: 22s - loss: 8.6677 - acc: 0.46 - ETA: 20s - loss: 8.6499 - acc: 0.46 - ETA: 19s - loss: 8.6347 - acc: 0.46 - ETA: 18s - loss: 8.6299 - acc: 0.46 - ETA: 17s - loss: 8.6887 - acc: 0.46 - ETA: 16s - loss: 8.6442 - acc: 0.46 - ETA: 14s - loss: 8.7166 - acc: 0.45 - ETA: 13s - loss: 8.8080 - acc: 0.45 - ETA: 12s - loss: 8.8209 - acc: 0.45 - ETA: 11s - loss: 8.7966 - acc: 0.45 - ETA: 10s - loss: 8.7516 - acc: 0.45 - ETA: 8s - loss: 8.7598 - acc: 0.4565 - ETA: 7s - loss: 8.7097 - acc: 0.459 - ETA: 6s - loss: 8.7138 - acc: 0.459 - ETA: 5s - loss: 8.7516 - acc: 0.457 - ETA: 4s - loss: 8.7400 - acc: 0.457 - ETA: 2s - loss: 8.7471 - acc: 0.457 - ETA: 1s - loss: 8.7451 - acc: 0.457 - ETA: 0s - loss: 8.7390 - acc: 0.457 - 42s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 33/50\n",
      "3882/3882 [==============================] - ETA: 36s - loss: 8.4368 - acc: 0.47 - ETA: 33s - loss: 8.4368 - acc: 0.47 - ETA: 32s - loss: 8.5627 - acc: 0.46 - ETA: 31s - loss: 8.6887 - acc: 0.46 - ETA: 30s - loss: 8.6131 - acc: 0.46 - ETA: 29s - loss: 8.6467 - acc: 0.46 - ETA: 28s - loss: 8.9405 - acc: 0.44 - ETA: 27s - loss: 8.9090 - acc: 0.44 - ETA: 26s - loss: 8.8706 - acc: 0.44 - ETA: 24s - loss: 8.9657 - acc: 0.44 - ETA: 23s - loss: 8.8947 - acc: 0.44 - ETA: 22s - loss: 8.8461 - acc: 0.45 - ETA: 21s - loss: 8.8921 - acc: 0.44 - ETA: 20s - loss: 8.8775 - acc: 0.44 - ETA: 18s - loss: 8.8146 - acc: 0.45 - ETA: 17s - loss: 8.8382 - acc: 0.45 - ETA: 16s - loss: 8.7850 - acc: 0.45 - ETA: 15s - loss: 8.7726 - acc: 0.45 - ETA: 13s - loss: 8.8080 - acc: 0.45 - ETA: 12s - loss: 8.7390 - acc: 0.45 - ETA: 11s - loss: 8.7426 - acc: 0.45 - ETA: 10s - loss: 8.7631 - acc: 0.45 - ETA: 9s - loss: 8.7817 - acc: 0.4552 - ETA: 7s - loss: 8.7674 - acc: 0.456 - ETA: 6s - loss: 8.7844 - acc: 0.455 - ETA: 5s - loss: 8.7904 - acc: 0.454 - ETA: 4s - loss: 8.8053 - acc: 0.453 - ETA: 2s - loss: 8.8056 - acc: 0.453 - ETA: 1s - loss: 8.7798 - acc: 0.455 - ETA: 0s - loss: 8.7684 - acc: 0.456 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 34/50\n",
      "3882/3882 [==============================] - ETA: 36s - loss: 8.8146 - acc: 0.45 - ETA: 34s - loss: 8.1850 - acc: 0.49 - ETA: 33s - loss: 8.3109 - acc: 0.48 - ETA: 31s - loss: 8.3109 - acc: 0.48 - ETA: 30s - loss: 8.3613 - acc: 0.48 - ETA: 29s - loss: 8.4368 - acc: 0.47 - ETA: 28s - loss: 8.5088 - acc: 0.47 - ETA: 27s - loss: 8.6729 - acc: 0.46 - ETA: 25s - loss: 8.6747 - acc: 0.46 - ETA: 24s - loss: 8.7013 - acc: 0.46 - ETA: 23s - loss: 8.7573 - acc: 0.45 - ETA: 22s - loss: 8.7621 - acc: 0.45 - ETA: 21s - loss: 8.7177 - acc: 0.45 - ETA: 19s - loss: 8.6527 - acc: 0.46 - ETA: 18s - loss: 8.7558 - acc: 0.45 - ETA: 17s - loss: 8.7359 - acc: 0.45 - ETA: 16s - loss: 8.7183 - acc: 0.45 - ETA: 15s - loss: 8.7516 - acc: 0.45 - ETA: 13s - loss: 8.7748 - acc: 0.45 - ETA: 12s - loss: 8.7705 - acc: 0.45 - ETA: 11s - loss: 8.7546 - acc: 0.45 - ETA: 10s - loss: 8.7631 - acc: 0.45 - ETA: 9s - loss: 8.7489 - acc: 0.4572 - ETA: 7s - loss: 8.7779 - acc: 0.455 - ETA: 6s - loss: 8.7743 - acc: 0.455 - ETA: 5s - loss: 8.7613 - acc: 0.456 - ETA: 4s - loss: 8.7819 - acc: 0.455 - ETA: 2s - loss: 8.7381 - acc: 0.457 - ETA: 1s - loss: 8.7364 - acc: 0.458 - ETA: 0s - loss: 8.7432 - acc: 0.457 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 35/50\n",
      "3882/3882 [==============================] - ETA: 37s - loss: 8.5627 - acc: 0.46 - ETA: 35s - loss: 8.5627 - acc: 0.46 - ETA: 34s - loss: 8.1430 - acc: 0.49 - ETA: 32s - loss: 8.3424 - acc: 0.48 - ETA: 31s - loss: 8.3613 - acc: 0.48 - ETA: 30s - loss: 8.3948 - acc: 0.47 - ETA: 28s - loss: 8.4188 - acc: 0.47 - ETA: 27s - loss: 8.4053 - acc: 0.47 - ETA: 26s - loss: 8.2829 - acc: 0.48 - ETA: 25s - loss: 8.2731 - acc: 0.48 - ETA: 23s - loss: 8.2766 - acc: 0.48 - ETA: 22s - loss: 8.2899 - acc: 0.48 - ETA: 21s - loss: 8.3303 - acc: 0.48 - ETA: 20s - loss: 8.3109 - acc: 0.48 - ETA: 18s - loss: 8.3277 - acc: 0.48 - ETA: 17s - loss: 8.3739 - acc: 0.48 - ETA: 16s - loss: 8.3998 - acc: 0.47 - ETA: 15s - loss: 8.4368 - acc: 0.47 - ETA: 14s - loss: 8.4700 - acc: 0.47 - ETA: 12s - loss: 8.4998 - acc: 0.47 - ETA: 11s - loss: 8.5088 - acc: 0.47 - ETA: 10s - loss: 8.5799 - acc: 0.46 - ETA: 9s - loss: 8.6394 - acc: 0.4640 - ETA: 7s - loss: 8.6414 - acc: 0.463 - ETA: 6s - loss: 8.6383 - acc: 0.464 - ETA: 5s - loss: 8.6741 - acc: 0.461 - ETA: 4s - loss: 8.6793 - acc: 0.461 - ETA: 2s - loss: 8.7067 - acc: 0.459 - ETA: 1s - loss: 8.7104 - acc: 0.459 - ETA: 0s - loss: 8.7432 - acc: 0.457 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 36/50\n",
      "3882/3882 [==============================] - ETA: 38s - loss: 8.5627 - acc: 0.46 - ETA: 35s - loss: 8.8775 - acc: 0.44 - ETA: 34s - loss: 8.8146 - acc: 0.45 - ETA: 33s - loss: 8.6572 - acc: 0.46 - ETA: 31s - loss: 8.7642 - acc: 0.45 - ETA: 30s - loss: 8.7306 - acc: 0.45 - ETA: 28s - loss: 8.6167 - acc: 0.46 - ETA: 27s - loss: 8.6572 - acc: 0.46 - ETA: 26s - loss: 8.6467 - acc: 0.46 - ETA: 24s - loss: 8.5250 - acc: 0.47 - ETA: 23s - loss: 8.5170 - acc: 0.47 - ETA: 22s - loss: 8.6677 - acc: 0.46 - ETA: 21s - loss: 8.7758 - acc: 0.45 - ETA: 20s - loss: 8.8236 - acc: 0.45 - ETA: 18s - loss: 8.6887 - acc: 0.46 - ETA: 17s - loss: 8.6493 - acc: 0.46 - ETA: 16s - loss: 8.6146 - acc: 0.46 - ETA: 15s - loss: 8.5557 - acc: 0.46 - ETA: 13s - loss: 8.6158 - acc: 0.46 - ETA: 12s - loss: 8.6068 - acc: 0.46 - ETA: 11s - loss: 8.6647 - acc: 0.46 - ETA: 10s - loss: 8.6600 - acc: 0.46 - ETA: 9s - loss: 8.6613 - acc: 0.4626 - ETA: 7s - loss: 8.6362 - acc: 0.464 - ETA: 6s - loss: 8.6181 - acc: 0.465 - ETA: 5s - loss: 8.5966 - acc: 0.466 - ETA: 4s - loss: 8.6467 - acc: 0.463 - ETA: 2s - loss: 8.7022 - acc: 0.460 - ETA: 1s - loss: 8.6887 - acc: 0.460 - ETA: 0s - loss: 8.7138 - acc: 0.459 - 42s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3882/3882 [==============================] - ETA: 37s - loss: 9.3183 - acc: 0.42 - ETA: 35s - loss: 9.0664 - acc: 0.43 - ETA: 33s - loss: 8.8985 - acc: 0.44 - ETA: 32s - loss: 8.8146 - acc: 0.45 - ETA: 31s - loss: 8.7642 - acc: 0.45 - ETA: 30s - loss: 8.7306 - acc: 0.45 - ETA: 28s - loss: 8.7246 - acc: 0.45 - ETA: 27s - loss: 8.6572 - acc: 0.46 - ETA: 26s - loss: 8.7586 - acc: 0.45 - ETA: 25s - loss: 8.7013 - acc: 0.46 - ETA: 23s - loss: 8.7459 - acc: 0.45 - ETA: 22s - loss: 8.8356 - acc: 0.45 - ETA: 21s - loss: 8.8049 - acc: 0.45 - ETA: 20s - loss: 8.9045 - acc: 0.44 - ETA: 18s - loss: 8.9069 - acc: 0.44 - ETA: 17s - loss: 8.9169 - acc: 0.44 - ETA: 16s - loss: 8.9109 - acc: 0.44 - ETA: 15s - loss: 8.9545 - acc: 0.44 - ETA: 13s - loss: 8.9074 - acc: 0.44 - ETA: 12s - loss: 8.9342 - acc: 0.44 - ETA: 11s - loss: 8.8745 - acc: 0.44 - ETA: 10s - loss: 8.9004 - acc: 0.44 - ETA: 9s - loss: 8.8639 - acc: 0.4501 - ETA: 7s - loss: 8.8828 - acc: 0.448 - ETA: 6s - loss: 8.8599 - acc: 0.450 - ETA: 5s - loss: 8.7807 - acc: 0.455 - ETA: 4s - loss: 8.7400 - acc: 0.457 - ETA: 2s - loss: 8.7201 - acc: 0.459 - ETA: 1s - loss: 8.7191 - acc: 0.459 - ETA: 0s - loss: 8.7600 - acc: 0.456 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 38/50\n",
      "3882/3882 [==============================] - ETA: 35s - loss: 8.9405 - acc: 0.44 - ETA: 35s - loss: 8.9405 - acc: 0.44 - ETA: 33s - loss: 8.8146 - acc: 0.45 - ETA: 32s - loss: 8.7831 - acc: 0.45 - ETA: 31s - loss: 8.9405 - acc: 0.44 - ETA: 30s - loss: 8.8775 - acc: 0.44 - ETA: 28s - loss: 8.7606 - acc: 0.45 - ETA: 27s - loss: 8.6572 - acc: 0.46 - ETA: 26s - loss: 8.8146 - acc: 0.45 - ETA: 24s - loss: 8.8524 - acc: 0.45 - ETA: 23s - loss: 8.7917 - acc: 0.45 - ETA: 22s - loss: 8.8566 - acc: 0.45 - ETA: 21s - loss: 8.8921 - acc: 0.44 - ETA: 19s - loss: 8.8506 - acc: 0.45 - ETA: 18s - loss: 8.8314 - acc: 0.45 - ETA: 17s - loss: 8.8461 - acc: 0.45 - ETA: 16s - loss: 8.8738 - acc: 0.44 - ETA: 15s - loss: 8.8566 - acc: 0.45 - ETA: 13s - loss: 8.8212 - acc: 0.45 - ETA: 12s - loss: 8.8398 - acc: 0.45 - ETA: 11s - loss: 8.8566 - acc: 0.45 - ETA: 10s - loss: 8.8661 - acc: 0.44 - ETA: 8s - loss: 8.8584 - acc: 0.4504 - ETA: 7s - loss: 8.8356 - acc: 0.451 - ETA: 6s - loss: 8.8398 - acc: 0.451 - ETA: 5s - loss: 8.8243 - acc: 0.452 - ETA: 4s - loss: 8.7959 - acc: 0.454 - ETA: 2s - loss: 8.7291 - acc: 0.458 - ETA: 1s - loss: 8.7191 - acc: 0.459 - ETA: 0s - loss: 8.7516 - acc: 0.457 - 42s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 39/50\n",
      "3882/3882 [==============================] - ETA: 36s - loss: 7.5554 - acc: 0.53 - ETA: 35s - loss: 8.3109 - acc: 0.48 - ETA: 34s - loss: 8.3109 - acc: 0.48 - ETA: 32s - loss: 8.3109 - acc: 0.48 - ETA: 31s - loss: 8.6131 - acc: 0.46 - ETA: 29s - loss: 8.6467 - acc: 0.46 - ETA: 28s - loss: 8.6167 - acc: 0.46 - ETA: 27s - loss: 8.5627 - acc: 0.46 - ETA: 26s - loss: 8.5348 - acc: 0.47 - ETA: 24s - loss: 8.6761 - acc: 0.46 - ETA: 23s - loss: 8.6543 - acc: 0.46 - ETA: 22s - loss: 8.6467 - acc: 0.46 - ETA: 21s - loss: 8.6499 - acc: 0.46 - ETA: 19s - loss: 8.6257 - acc: 0.46 - ETA: 18s - loss: 8.6131 - acc: 0.46 - ETA: 17s - loss: 8.5785 - acc: 0.46 - ETA: 16s - loss: 8.5701 - acc: 0.46 - ETA: 15s - loss: 8.6047 - acc: 0.46 - ETA: 13s - loss: 8.6290 - acc: 0.46 - ETA: 12s - loss: 8.6257 - acc: 0.46 - ETA: 11s - loss: 8.6347 - acc: 0.46 - ETA: 10s - loss: 8.7116 - acc: 0.45 - ETA: 8s - loss: 8.7927 - acc: 0.4545 - ETA: 7s - loss: 8.7201 - acc: 0.459 - ETA: 6s - loss: 8.7290 - acc: 0.458 - ETA: 5s - loss: 8.7516 - acc: 0.457 - ETA: 4s - loss: 8.7540 - acc: 0.456 - ETA: 2s - loss: 8.7336 - acc: 0.458 - ETA: 1s - loss: 8.7712 - acc: 0.455 - ETA: 0s - loss: 8.7516 - acc: 0.457 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 40/50\n",
      "3882/3882 [==============================] - ETA: 36s - loss: 7.9331 - acc: 0.50 - ETA: 36s - loss: 8.2479 - acc: 0.48 - ETA: 33s - loss: 8.4368 - acc: 0.47 - ETA: 32s - loss: 8.2794 - acc: 0.48 - ETA: 31s - loss: 8.3613 - acc: 0.48 - ETA: 30s - loss: 8.5627 - acc: 0.46 - ETA: 29s - loss: 8.7067 - acc: 0.45 - ETA: 27s - loss: 8.8933 - acc: 0.44 - ETA: 26s - loss: 8.9265 - acc: 0.44 - ETA: 25s - loss: 8.9153 - acc: 0.44 - ETA: 24s - loss: 8.9062 - acc: 0.44 - ETA: 23s - loss: 8.9300 - acc: 0.44 - ETA: 21s - loss: 8.8146 - acc: 0.45 - ETA: 20s - loss: 8.7786 - acc: 0.45 - ETA: 19s - loss: 8.8985 - acc: 0.44 - ETA: 17s - loss: 8.9326 - acc: 0.44 - ETA: 16s - loss: 8.9035 - acc: 0.44 - ETA: 15s - loss: 8.8566 - acc: 0.45 - ETA: 14s - loss: 8.8278 - acc: 0.45 - ETA: 12s - loss: 8.7768 - acc: 0.45 - ETA: 11s - loss: 8.7966 - acc: 0.45 - ETA: 10s - loss: 8.7802 - acc: 0.45 - ETA: 9s - loss: 8.7598 - acc: 0.4565 - ETA: 7s - loss: 8.7726 - acc: 0.455 - ETA: 6s - loss: 8.7693 - acc: 0.455 - ETA: 5s - loss: 8.7613 - acc: 0.456 - ETA: 4s - loss: 8.7446 - acc: 0.457 - ETA: 2s - loss: 8.6842 - acc: 0.461 - ETA: 1s - loss: 8.6843 - acc: 0.461 - ETA: 0s - loss: 8.7222 - acc: 0.458 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 41/50\n",
      "3882/3882 [==============================] - ETA: 36s - loss: 10.4516 - acc: 0.351 - ETA: 34s - loss: 9.5701 - acc: 0.406 - ETA: 33s - loss: 9.3603 - acc: 0.41 - ETA: 32s - loss: 9.5386 - acc: 0.40 - ETA: 30s - loss: 9.3435 - acc: 0.42 - ETA: 29s - loss: 9.2343 - acc: 0.42 - ETA: 28s - loss: 8.9765 - acc: 0.44 - ETA: 27s - loss: 8.9405 - acc: 0.44 - ETA: 26s - loss: 8.8845 - acc: 0.44 - ETA: 24s - loss: 8.7516 - acc: 0.45 - ETA: 23s - loss: 8.5742 - acc: 0.46 - ETA: 22s - loss: 8.6362 - acc: 0.46 - ETA: 21s - loss: 8.5918 - acc: 0.46 - ETA: 19s - loss: 8.6707 - acc: 0.46 - ETA: 18s - loss: 8.6719 - acc: 0.46 - ETA: 17s - loss: 8.6257 - acc: 0.46 - ETA: 16s - loss: 8.6590 - acc: 0.46 - ETA: 15s - loss: 8.6257 - acc: 0.46 - ETA: 13s - loss: 8.6622 - acc: 0.46 - ETA: 12s - loss: 8.6635 - acc: 0.46 - ETA: 11s - loss: 8.6107 - acc: 0.46 - ETA: 10s - loss: 8.6600 - acc: 0.46 - ETA: 8s - loss: 8.6941 - acc: 0.4606 - ETA: 7s - loss: 8.6887 - acc: 0.460 - ETA: 6s - loss: 8.7290 - acc: 0.458 - ETA: 5s - loss: 8.7080 - acc: 0.459 - ETA: 4s - loss: 8.7166 - acc: 0.459 - ETA: 2s - loss: 8.7291 - acc: 0.458 - ETA: 1s - loss: 8.7104 - acc: 0.459 - ETA: 0s - loss: 8.7516 - acc: 0.457 - 42s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 42/50\n",
      "3882/3882 [==============================] - ETA: 36s - loss: 9.3183 - acc: 0.42 - ETA: 34s - loss: 9.1294 - acc: 0.43 - ETA: 33s - loss: 9.1924 - acc: 0.42 - ETA: 32s - loss: 8.9090 - acc: 0.44 - ETA: 30s - loss: 9.1420 - acc: 0.43 - ETA: 29s - loss: 9.0664 - acc: 0.43 - ETA: 28s - loss: 9.1924 - acc: 0.42 - ETA: 27s - loss: 9.1609 - acc: 0.43 - ETA: 25s - loss: 9.0804 - acc: 0.43 - ETA: 24s - loss: 9.0538 - acc: 0.43 - ETA: 23s - loss: 8.9863 - acc: 0.44 - ETA: 22s - loss: 8.9510 - acc: 0.44 - ETA: 21s - loss: 9.0567 - acc: 0.43 - ETA: 19s - loss: 9.0484 - acc: 0.43 - ETA: 18s - loss: 9.0245 - acc: 0.44 - ETA: 17s - loss: 9.0113 - acc: 0.44 - ETA: 16s - loss: 8.9701 - acc: 0.44 - ETA: 15s - loss: 8.9265 - acc: 0.44 - ETA: 13s - loss: 8.8875 - acc: 0.44 - ETA: 12s - loss: 8.8461 - acc: 0.45 - ETA: 11s - loss: 8.8446 - acc: 0.45 - ETA: 10s - loss: 8.8547 - acc: 0.45 - ETA: 8s - loss: 8.8529 - acc: 0.4507 - ETA: 7s - loss: 8.8198 - acc: 0.452 - ETA: 6s - loss: 8.8146 - acc: 0.453 - ETA: 5s - loss: 8.7904 - acc: 0.454 - ETA: 4s - loss: 8.7353 - acc: 0.458 - ETA: 2s - loss: 8.7606 - acc: 0.456 - ETA: 1s - loss: 8.7885 - acc: 0.454 - ETA: 0s - loss: 8.7642 - acc: 0.456 - 42s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3882/3882 [==============================] - ETA: 35s - loss: 9.6960 - acc: 0.39 - ETA: 35s - loss: 8.8146 - acc: 0.45 - ETA: 34s - loss: 8.8566 - acc: 0.45 - ETA: 32s - loss: 8.9720 - acc: 0.44 - ETA: 31s - loss: 8.8146 - acc: 0.45 - ETA: 30s - loss: 8.8146 - acc: 0.45 - ETA: 28s - loss: 8.7426 - acc: 0.45 - ETA: 27s - loss: 8.7359 - acc: 0.45 - ETA: 26s - loss: 8.8006 - acc: 0.45 - ETA: 25s - loss: 8.7642 - acc: 0.45 - ETA: 23s - loss: 8.7116 - acc: 0.45 - ETA: 22s - loss: 8.7097 - acc: 0.45 - ETA: 21s - loss: 8.6112 - acc: 0.46 - ETA: 20s - loss: 8.5717 - acc: 0.46 - ETA: 19s - loss: 8.5460 - acc: 0.46 - ETA: 17s - loss: 8.5627 - acc: 0.46 - ETA: 16s - loss: 8.5257 - acc: 0.47 - ETA: 15s - loss: 8.5697 - acc: 0.46 - ETA: 14s - loss: 8.5627 - acc: 0.46 - ETA: 12s - loss: 8.6068 - acc: 0.46 - ETA: 11s - loss: 8.6827 - acc: 0.46 - ETA: 10s - loss: 8.7173 - acc: 0.45 - ETA: 9s - loss: 8.7379 - acc: 0.4579 - ETA: 7s - loss: 8.7569 - acc: 0.456 - ETA: 6s - loss: 8.7592 - acc: 0.456 - ETA: 5s - loss: 8.7807 - acc: 0.455 - ETA: 4s - loss: 8.7493 - acc: 0.457 - ETA: 2s - loss: 8.7336 - acc: 0.458 - ETA: 1s - loss: 8.6930 - acc: 0.460 - ETA: 0s - loss: 8.7474 - acc: 0.457 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 44/50\n",
      "3882/3882 [==============================] - ETA: 36s - loss: 8.3109 - acc: 0.48 - ETA: 35s - loss: 8.1850 - acc: 0.49 - ETA: 33s - loss: 8.1430 - acc: 0.49 - ETA: 32s - loss: 8.0590 - acc: 0.50 - ETA: 31s - loss: 8.2102 - acc: 0.49 - ETA: 29s - loss: 8.3109 - acc: 0.48 - ETA: 28s - loss: 8.0950 - acc: 0.49 - ETA: 27s - loss: 8.2322 - acc: 0.48 - ETA: 26s - loss: 8.3529 - acc: 0.48 - ETA: 24s - loss: 8.3739 - acc: 0.48 - ETA: 23s - loss: 8.4826 - acc: 0.47 - ETA: 22s - loss: 8.4788 - acc: 0.47 - ETA: 21s - loss: 8.5434 - acc: 0.47 - ETA: 20s - loss: 8.6077 - acc: 0.46 - ETA: 18s - loss: 8.6551 - acc: 0.46 - ETA: 17s - loss: 8.6965 - acc: 0.46 - ETA: 16s - loss: 8.6294 - acc: 0.46 - ETA: 15s - loss: 8.6887 - acc: 0.46 - ETA: 13s - loss: 8.6887 - acc: 0.46 - ETA: 12s - loss: 8.7201 - acc: 0.45 - ETA: 11s - loss: 8.7126 - acc: 0.45 - ETA: 10s - loss: 8.7001 - acc: 0.46 - ETA: 8s - loss: 8.6777 - acc: 0.4616 - ETA: 7s - loss: 8.6572 - acc: 0.462 - ETA: 6s - loss: 8.6786 - acc: 0.461 - ETA: 5s - loss: 8.6741 - acc: 0.461 - ETA: 4s - loss: 8.7027 - acc: 0.460 - ETA: 2s - loss: 8.7471 - acc: 0.457 - ETA: 1s - loss: 8.7408 - acc: 0.457 - ETA: 0s - loss: 8.7390 - acc: 0.457 - 42s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 45/50\n",
      "3882/3882 [==============================] - ETA: 34s - loss: 8.6887 - acc: 0.46 - ETA: 34s - loss: 8.4368 - acc: 0.47 - ETA: 33s - loss: 8.3948 - acc: 0.47 - ETA: 32s - loss: 8.5627 - acc: 0.46 - ETA: 31s - loss: 8.6887 - acc: 0.46 - ETA: 30s - loss: 8.5837 - acc: 0.46 - ETA: 29s - loss: 8.6167 - acc: 0.46 - ETA: 27s - loss: 8.6257 - acc: 0.46 - ETA: 26s - loss: 8.6467 - acc: 0.46 - ETA: 25s - loss: 8.6635 - acc: 0.46 - ETA: 23s - loss: 8.6543 - acc: 0.46 - ETA: 22s - loss: 8.7201 - acc: 0.45 - ETA: 21s - loss: 8.6983 - acc: 0.46 - ETA: 20s - loss: 8.6527 - acc: 0.46 - ETA: 19s - loss: 8.6047 - acc: 0.46 - ETA: 17s - loss: 8.6021 - acc: 0.46 - ETA: 16s - loss: 8.5998 - acc: 0.46 - ETA: 15s - loss: 8.6117 - acc: 0.46 - ETA: 14s - loss: 8.6224 - acc: 0.46 - ETA: 12s - loss: 8.6635 - acc: 0.46 - ETA: 11s - loss: 8.6587 - acc: 0.46 - ETA: 10s - loss: 8.6772 - acc: 0.46 - ETA: 9s - loss: 8.6668 - acc: 0.4623 - ETA: 7s - loss: 8.6782 - acc: 0.461 - ETA: 6s - loss: 8.7189 - acc: 0.459 - ETA: 5s - loss: 8.7226 - acc: 0.458 - ETA: 4s - loss: 8.6887 - acc: 0.460 - ETA: 2s - loss: 8.7022 - acc: 0.460 - ETA: 1s - loss: 8.7060 - acc: 0.459 - ETA: 0s - loss: 8.7390 - acc: 0.457 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 46/50\n",
      "3882/3882 [==============================] - ETA: 36s - loss: 9.3183 - acc: 0.42 - ETA: 34s - loss: 8.6257 - acc: 0.46 - ETA: 33s - loss: 8.6047 - acc: 0.46 - ETA: 32s - loss: 8.6572 - acc: 0.46 - ETA: 31s - loss: 8.5124 - acc: 0.47 - ETA: 30s - loss: 8.7306 - acc: 0.45 - ETA: 28s - loss: 8.7067 - acc: 0.45 - ETA: 27s - loss: 8.6414 - acc: 0.46 - ETA: 26s - loss: 8.6747 - acc: 0.46 - ETA: 25s - loss: 8.6761 - acc: 0.46 - ETA: 23s - loss: 8.7001 - acc: 0.46 - ETA: 22s - loss: 8.6887 - acc: 0.46 - ETA: 21s - loss: 8.8049 - acc: 0.45 - ETA: 20s - loss: 8.7966 - acc: 0.45 - ETA: 18s - loss: 8.8230 - acc: 0.45 - ETA: 17s - loss: 8.7831 - acc: 0.45 - ETA: 16s - loss: 8.7775 - acc: 0.45 - ETA: 15s - loss: 8.7306 - acc: 0.45 - ETA: 13s - loss: 8.7085 - acc: 0.45 - ETA: 12s - loss: 8.6887 - acc: 0.46 - ETA: 11s - loss: 8.6947 - acc: 0.46 - ETA: 10s - loss: 8.7287 - acc: 0.45 - ETA: 9s - loss: 8.7379 - acc: 0.4579 - ETA: 7s - loss: 8.7149 - acc: 0.459 - ETA: 6s - loss: 8.7390 - acc: 0.457 - ETA: 5s - loss: 8.6983 - acc: 0.460 - ETA: 4s - loss: 8.7633 - acc: 0.456 - ETA: 2s - loss: 8.7291 - acc: 0.458 - ETA: 1s - loss: 8.7364 - acc: 0.458 - ETA: 0s - loss: 8.7516 - acc: 0.457 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 47/50\n",
      "3882/3882 [==============================] - ETA: 35s - loss: 8.1850 - acc: 0.49 - ETA: 34s - loss: 8.2479 - acc: 0.48 - ETA: 33s - loss: 8.0590 - acc: 0.50 - ETA: 31s - loss: 7.9961 - acc: 0.50 - ETA: 30s - loss: 8.3109 - acc: 0.48 - ETA: 29s - loss: 8.4578 - acc: 0.47 - ETA: 27s - loss: 8.6167 - acc: 0.46 - ETA: 26s - loss: 8.7516 - acc: 0.45 - ETA: 25s - loss: 8.8006 - acc: 0.45 - ETA: 24s - loss: 8.8524 - acc: 0.45 - ETA: 23s - loss: 8.8260 - acc: 0.45 - ETA: 22s - loss: 8.8146 - acc: 0.45 - ETA: 20s - loss: 8.7758 - acc: 0.45 - ETA: 19s - loss: 8.7966 - acc: 0.45 - ETA: 18s - loss: 8.7474 - acc: 0.45 - ETA: 17s - loss: 8.7359 - acc: 0.45 - ETA: 16s - loss: 8.7775 - acc: 0.45 - ETA: 14s - loss: 8.7306 - acc: 0.45 - ETA: 13s - loss: 8.6953 - acc: 0.46 - ETA: 12s - loss: 8.6383 - acc: 0.46 - ETA: 11s - loss: 8.6347 - acc: 0.46 - ETA: 10s - loss: 8.7116 - acc: 0.45 - ETA: 8s - loss: 8.7215 - acc: 0.4589 - ETA: 7s - loss: 8.7936 - acc: 0.454 - ETA: 6s - loss: 8.7743 - acc: 0.455 - ETA: 5s - loss: 8.7419 - acc: 0.457 - ETA: 4s - loss: 8.7633 - acc: 0.456 - ETA: 2s - loss: 8.7741 - acc: 0.455 - ETA: 1s - loss: 8.7755 - acc: 0.455 - ETA: 0s - loss: 8.7684 - acc: 0.456 - 42s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 48/50\n",
      "3882/3882 [==============================] - ETA: 36s - loss: 8.8146 - acc: 0.45 - ETA: 35s - loss: 8.5627 - acc: 0.46 - ETA: 33s - loss: 8.1850 - acc: 0.49 - ETA: 32s - loss: 8.5942 - acc: 0.46 - ETA: 32s - loss: 8.6131 - acc: 0.46 - ETA: 30s - loss: 8.7306 - acc: 0.45 - ETA: 29s - loss: 8.6527 - acc: 0.46 - ETA: 27s - loss: 8.5313 - acc: 0.47 - ETA: 26s - loss: 8.5767 - acc: 0.46 - ETA: 25s - loss: 8.5376 - acc: 0.47 - ETA: 23s - loss: 8.6429 - acc: 0.46 - ETA: 22s - loss: 8.6782 - acc: 0.46 - ETA: 21s - loss: 8.6693 - acc: 0.46 - ETA: 20s - loss: 8.6977 - acc: 0.46 - ETA: 18s - loss: 8.6719 - acc: 0.46 - ETA: 17s - loss: 8.6021 - acc: 0.46 - ETA: 16s - loss: 8.7035 - acc: 0.46 - ETA: 15s - loss: 8.7586 - acc: 0.45 - ETA: 13s - loss: 8.7483 - acc: 0.45 - ETA: 12s - loss: 8.7327 - acc: 0.45 - ETA: 11s - loss: 8.7246 - acc: 0.45 - ETA: 10s - loss: 8.7459 - acc: 0.45 - ETA: 9s - loss: 8.7598 - acc: 0.4565 - ETA: 7s - loss: 8.7201 - acc: 0.459 - ETA: 6s - loss: 8.7592 - acc: 0.456 - ETA: 5s - loss: 8.7613 - acc: 0.456 - ETA: 4s - loss: 8.7819 - acc: 0.455 - ETA: 2s - loss: 8.7651 - acc: 0.456 - ETA: 1s - loss: 8.7364 - acc: 0.458 - ETA: 0s - loss: 8.7264 - acc: 0.458 - 43s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3882/3882 [==============================] - ETA: 35s - loss: 9.8220 - acc: 0.39 - ETA: 34s - loss: 8.7516 - acc: 0.45 - ETA: 33s - loss: 9.0245 - acc: 0.44 - ETA: 32s - loss: 8.9090 - acc: 0.44 - ETA: 31s - loss: 8.7390 - acc: 0.45 - ETA: 29s - loss: 8.7516 - acc: 0.45 - ETA: 28s - loss: 8.6707 - acc: 0.46 - ETA: 27s - loss: 8.7674 - acc: 0.45 - ETA: 26s - loss: 8.7166 - acc: 0.45 - ETA: 24s - loss: 8.8020 - acc: 0.45 - ETA: 23s - loss: 8.8375 - acc: 0.45 - ETA: 22s - loss: 8.8671 - acc: 0.44 - ETA: 21s - loss: 8.7855 - acc: 0.45 - ETA: 19s - loss: 8.6887 - acc: 0.46 - ETA: 18s - loss: 8.6803 - acc: 0.46 - ETA: 17s - loss: 8.7123 - acc: 0.45 - ETA: 16s - loss: 8.6813 - acc: 0.46 - ETA: 15s - loss: 8.6747 - acc: 0.46 - ETA: 13s - loss: 8.6754 - acc: 0.46 - ETA: 12s - loss: 8.7013 - acc: 0.46 - ETA: 11s - loss: 8.7186 - acc: 0.45 - ETA: 10s - loss: 8.7459 - acc: 0.45 - ETA: 9s - loss: 8.7598 - acc: 0.4565 - ETA: 7s - loss: 8.7516 - acc: 0.457 - ETA: 6s - loss: 8.7290 - acc: 0.458 - ETA: 5s - loss: 8.7419 - acc: 0.457 - ETA: 4s - loss: 8.7493 - acc: 0.457 - ETA: 2s - loss: 8.7516 - acc: 0.457 - ETA: 1s - loss: 8.7755 - acc: 0.455 - ETA: 0s - loss: 8.7474 - acc: 0.457 - 42s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n",
      "Epoch 50/50\n",
      "3882/3882 [==============================] - ETA: 37s - loss: 8.8146 - acc: 0.45 - ETA: 34s - loss: 8.5627 - acc: 0.46 - ETA: 33s - loss: 8.6887 - acc: 0.46 - ETA: 32s - loss: 8.6572 - acc: 0.46 - ETA: 30s - loss: 8.6383 - acc: 0.46 - ETA: 29s - loss: 8.6257 - acc: 0.46 - ETA: 28s - loss: 8.6707 - acc: 0.46 - ETA: 27s - loss: 8.7201 - acc: 0.45 - ETA: 26s - loss: 8.8006 - acc: 0.45 - ETA: 24s - loss: 8.8020 - acc: 0.45 - ETA: 23s - loss: 8.7345 - acc: 0.45 - ETA: 22s - loss: 8.6887 - acc: 0.46 - ETA: 21s - loss: 8.6112 - acc: 0.46 - ETA: 19s - loss: 8.6527 - acc: 0.46 - ETA: 18s - loss: 8.7055 - acc: 0.45 - ETA: 17s - loss: 8.7831 - acc: 0.45 - ETA: 16s - loss: 8.7701 - acc: 0.45 - ETA: 15s - loss: 8.7936 - acc: 0.45 - ETA: 13s - loss: 8.7947 - acc: 0.45 - ETA: 12s - loss: 8.7957 - acc: 0.45 - ETA: 11s - loss: 8.8326 - acc: 0.45 - ETA: 10s - loss: 8.7631 - acc: 0.45 - ETA: 9s - loss: 8.7434 - acc: 0.4575 - ETA: 7s - loss: 8.7359 - acc: 0.458 - ETA: 6s - loss: 8.7491 - acc: 0.457 - ETA: 5s - loss: 8.7419 - acc: 0.457 - ETA: 4s - loss: 8.7446 - acc: 0.457 - ETA: 2s - loss: 8.7336 - acc: 0.458 - ETA: 1s - loss: 8.7885 - acc: 0.454 - ETA: 0s - loss: 8.7642 - acc: 0.456 - 41s 11ms/step - loss: 8.7441 - acc: 0.4575 - val_loss: 8.7192 - val_acc: 0.4590\n"
     ]
    }
   ],
   "source": [
    "history1 = model1.fit(X_train, train_labels, batch_size=batch_size, epochs=epochs, validation_split=0.25, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = history1.history['loss']\n",
    "val_loss = history1.history['val_loss']\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss, 'blue', label='Training Loss')\n",
    "plt.plot(val_loss, 'green', label='Validation Loss')\n",
    "plt.xticks(range(0,epochs)[0::2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Test Data ********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MMD\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:182: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  score = y_true == y_pred\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mix of label input types (string and number)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-3bd67bb80515>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy: \"\u001b[0m \u001b[1;33m+\u001b[0m  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mmodel_accuracy_comparison\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CNN Model 1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Print confusion matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict)\u001b[0m\n\u001b[0;32m   1526\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1527\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1528\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1529\u001b[0m         \u001b[0mlabels_given\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;31m# Check that we don't mix string type with number type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mys_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mix of label input types (string and number)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Mix of label input types (string and number)"
     ]
    }
   ],
   "source": [
    "# Make a set of predictions for the validation data\n",
    "pred = model1.predict_classes(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, pred) \n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, pred))\n",
    "model_accuracy_comparison['CNN Model 1'] = accuracy\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "print(metrics.confusion_matrix(test_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
